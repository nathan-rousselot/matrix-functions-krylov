%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Problem Set/Assignment Template to be used by the
%% Food and Resource Economics Department - IFAS
%% University of Florida's graduates.
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Version 1.0 - November 2019
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Ariel Soto-Caro
%%  - asotocaro@ufl.edu
%%  - arielsotocaro@gmail.com
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[11pt]{article}
\usepackage{float}
\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
%\usepackage{algorithm}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{tikz}
\usepackage{titling}
\renewcommand\maketitlehooka{\null\mbox{}\vfill}
\renewcommand\maketitlehookd{\vfill\null}
\usepackage[a4paper, total={6.5in, 9in}]{geometry}
\usepackage[style=authoryear, backend=biber]{biblatex}
\bibliography{bibliography.bib}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
   \node[shape=circle,draw=red,inner sep=1pt] (char) {#1};}}
\setlength\parindent{0pt} %% Do not touch this
\DeclareMathOperator{\phiAb}{\phi_{\mathbf{A},\mathbf{b}}}
\DeclareMathOperator{\spn}{span}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}{Theorem}[section]
\newtheorem{definition}{Definition}[section]
\newtheorem{corollary}{Corollary}[theorem]
\newtheorem{example}{Example}[section]
\newtheorem{remark}{Remark}[section]

\usepackage{pgfplots}
\pgfplotsset{compat=newest}
  %% the following commands are needed for some matlab2tikz features
\usetikzlibrary{plotmarks}
\usetikzlibrary{arrows.meta}
\usepgfplotslibrary{patchplots}
\usepackage{grffile}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{hyperref}
\setcounter{secnumdepth}{4}
\usepgfplotslibrary{external} 
\tikzexternalize


\numberwithin{equation}{section}

%% -----------------------------
%% TITLE
%% -----------------------------
\title{Note on Matrix functions} %% Assignment Title
\author{Nathan Rousselot}
%% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%\setlength{\droptitle}{-5em}    
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlingpage}
    \maketitle
\end{titlingpage}
\tableofcontents
\newpage

\section{Introduction}
The study of matrix functions is an important concept in numerical linear algebra with critical applications spanning multiple disciplines, from physical simulations by solving ODEs and PDEs (\cite{higham2008functions}) to control theory, and passing by more advanced algebra concepts such that Volterra Kernels where matrix exponential plays a key role (\cite{burt2017efficient}). Given a scalar function \( f: \mathbb{C} \rightarrow \mathbb{C} \), extending this function to act on matrices, \( f: \mathbb{C}^{n \times n} \rightarrow \mathbb{C}^{n \times n} \), is far from trivial. The extension raises fundamental mathematical and computational challenges, as matrices do not generally commute under multiplication and their spectral properties can vary widely. 

In this document, we first establish a formal mathematical foundation for matrix functions, rigorously defining how a function \( f \) can act upon a square matrix \( \mathbf{A} \). Second, we explore an elegant mathematical framework for evaluating any sufficiently differentiable function \( f \) on a matrix \( \mathbf{A} \). This approach leverages the spectral decomposition of \( \mathbf{A} \) and extends to more complex scenarios involving defective matrices.

Third, we delve into computational issues that manifest in the direct computation of matrix functions. A particular focus will be given to the role of Krylov subspaces (\cite{liesen2013krylov}) in avoiding these issues. It will be shown that the matrix-vector product \( f(\mathbf{A})\mathbf{b} \) can often be approximated more efficiently within a low-dimensional Krylov subspace, thus alleviating computational challenges without compromising accuracy significantly.

Finally, we present a series of numerical experiments to substantiate the theoretical assertions. These experiments are designed to validate the efficacy and limitations of different methods for computing matrix functions, particularly when exploiting Krylov subspaces for computational efficiency.

Our approach provides not only a comprehensive understanding of matrix functions but also offers practical computational strategies crucial for large-scale problems in science and engineering.


\section{Theoretical background}
In this section, we will see how we can define a matrix function.
\subsection{Natural Definition}
\subsubsection*{Polynomial functions}
Let $p:\mathbb{C}\rightarrow\mathbb{C}$ be a polynomial function of degree $d$:
\begin{equation}
    p(t) = \sum_{k=0}^d c_k t^k
\end{equation}
Then, considering a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and posing $\mathbf{A}^0 = I_n$, we can define the polynomial function $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ on a matrix $\mathbf{A}$ as follows:
\begin{equation}
    p(\mathbf{A}) = \sum_{k=0}^d c_k \mathbf{A}^k
\end{equation}

\subsubsection*{Rational functions}
Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a rational function of the form:
\begin{equation}
    f(t) := \frac{p(t)}{q(t)}
\end{equation}
It is not immediate how one would approach this function with a matrix. We want to define
\begin{equation}\label{eq:rationale}
    f(\mathbf{A}) := q(\mathbf{A})^{-1}p(\mathbf{A})
\end{equation}
However, this is not well defined if $q(\mathbf{A})$ is singular. In other words, we need to make sure that $q(\mathbf{A})$ is invertible. This is the case if and only if $q(\lambda) \neq 0$ for all eigenvalues $\lambda$ of $\mathbf{A}$. This is a very strong condition, and it is not always possible to find a rational function $f$ such that $q(\lambda) \neq 0$ for all eigenvalues $\lambda$ of $\mathbf{A}$. We note that a choice in notation has been made in equation \ref{eq:rationale}, though we can note that communation holds, thus the other notation is still valid.
\begin{lemma}\label{lem:commute}
     Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and let $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ and $q:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$, two matrix polynomials. Then,
     \begin{equation}
         q(\mathbf{A})p(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})
     \end{equation}
\end{lemma}
\begin{proof}
    Let $f(\mathbf{A}) := q(\mathbf{A})p(\mathbf{A})$, and $g(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})$. Then
    \begin{align*}
        f(\mathbf{A}) = \left(\sum_{k=0}^d c_k\mathbf{A}^k\right)\left(\sum_{j=0}^m b_j\mathbf{A}^j\right)\\
        = \sum_{k=0}^d \sum_{j=0}^m c_kb_j\mathbf{A}^{j+k}\\
        = \sum_{j=0}^m \sum_{k=0}^d b_jc_k\mathbf{A}^{k+j}\\
        = \left(\sum_{j=0}^m b_j\mathbf{A}^j\right)\left(\sum_{k=0}^d c_k\mathbf{A}^k\right) = g(\mathbf{A})
    \end{align*}
\end{proof}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and let $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ and $q:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$, two matrix polynomials such that $q(\mathbf{A})$ is non-singular. Then, 
    \begin{equation}
        q(\mathbf{A})^{-1}p(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})^{-1}
    \end{equation}
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a matrix such that $q(\mathbf{A})$ is non-singular
    \begin{align*}
        q(\mathbf{A})^{-1}p(\mathbf{A}) = q(\mathbf{A})^{-1}p(\mathbf{A})q(\mathbf{A})q(\mathbf{A})^{-1} 
    \end{align*}
    Using Lemma \ref{lem:commute}
    \begin{align*}        
    q(\mathbf{A})^{-1}p(\mathbf{A})q(\mathbf{A})q(\mathbf{A})^{-1} = q(\mathbf{A})^{-1}q(\mathbf{A})p(\mathbf{A})q(\mathbf{A})^{-1} \\
        = p(\mathbf{A})q(\mathbf{A})^{-1}
    \end{align*}
\end{proof}

\subsubsection*{Power Series}
Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a function that can be expressed as a power series:
\begin{equation}
    f(t) = \sum_{k=0}^\infty c_k t^k
\end{equation}
Then, considering a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and posing $\mathbf{A}^0 = I_n$, we can define the power series function $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ on a matrix $\mathbf{A}$ as follows:
\begin{equation}
    f(\mathbf{A}) = \sum_{k=0}^\infty c_k \mathbf{A}^k
\end{equation}
In the scalar case, we know that the power series converges if $|t| < r$, where $r$ is the radius of convergence. Obviously this translates in the matrix power series
\begin{theorem}\label{th:power_convergence}
    Let $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ be a matrix power series. Then, the series converges if and only if $\rho(\mathbf{A}) < r$, where $\rho(\mathbf{A})$ is the spectral radius of $\mathbf{A}$, and $r$ is the radius of convergence of the scalar power series.
\end{theorem}
Proof is provided in \cite{frommer2008matrix}. In the case of a finite-order Laurent Series, \textit{i.e}:
\begin{equation}
    f(t) = \sum_{k=-d}^d c_k t^k
\end{equation}
For the matrix case, we need to ensure convergence (similarly to power series), but also ensure existance of the inverse, as Laurent series do have negative powers. If both of those conditions are satisfied, we can write the Laurent series as a matrix function:
\begin{equation}
    f(\mathbf{A}) = \sum_{k=-d}^d c_k \mathbf{A}^k
\end{equation}
\subsection{Spectrum-Based Definition}
\subsubsection*{Diagonalizable Matrices}
Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a diagonalizable matrix. That means that there exists a matrix $\mathbf{V}\in\mathbb{C}^{n\times n}$ such that $\mathbf{V}$ is invertible, and $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$, where $\mathbf{\Lambda}$ is a diagonal matrix containing the eigenvalues of $\mathbf{A}$ :
\begin{equation}
    \mathbf{\Lambda} = \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix}
\end{equation}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a diagonalizable matrix. Then we can define the function $f(\mathbf{A})$ as:
    \begin{equation}
        f(\mathbf{A}) := \mathbf{V}f(\mathbf{\Lambda})\mathbf{V}^{-1}
    \end{equation}
    with 
    \begin{equation}
        f(\mathbf{\Lambda}) = \begin{bmatrix}
            f(\lambda_1) & 0 & \cdots & 0 \\
            0 & f(\lambda_2) & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\lambda_n)
        \end{bmatrix}
    \end{equation}        
\end{theorem}
This property is very handy, as it allows us to compute matrix functions by simply applying the function to the eigenvalues of the matrix. Computationally, this avoids inverting matrices, and lots of matrix products. However, this property is only valid for diagonalizable matrices. This property puts constraints on $f(\mathbf{A})$, as its eigenvectors must form a basis $\mathbf{F}^n$. Another more practical constraint, that is sufficient but not necessary, is if $\mathbf{A}$ is a full rank matrix, then it is diagonalizable.

\subsubsection*{Defective Matrices}
In some cases, the matrix $\mathbf{A}$ is not diagonalizable, that means the sum of the dimensions of the eigenspaces is less than $n$, we call that a \textit{Defective Matrix}. In that case, we can generalize the principle of diagonalization using the Jordan canonical form of $\mathbf{A}$:
\begin{equation}
    \mathbf{A} = \mathbf{V}\mathbf{J}\mathbf{V}^{-1}
\end{equation}
where $\mathbf{J}$ is a Jordan matrix, and $\mathbf{V}$ is a matrix containing the generalized eigenvectors of $\mathbf{A}$. The Jordan matrix is a block diagonal matrix, where each block is a Jordan block. A Jordan block is a matrix of the form:
\begin{equation}
    \mathbf{J}_k(\lambda) = \begin{bmatrix}
        \lambda & 1 & 0 & \cdots & 0 \\
        0 & \lambda & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda & 1 \\
        0 & 0 & \cdots & 0 & \lambda
    \end{bmatrix}
\end{equation}
\begin{definition}\label{thm:jordan}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a defective matrix. Then we can define the function $f(\mathbf{A})$ as:
    \begin{equation}
        f(\mathbf{A}) := \mathbf{V}f(\mathbf{J})\mathbf{V}^{-1}
    \end{equation}
    with 
    \begin{equation}
        f(\mathbf{J}) = \begin{bmatrix}
            f(\mathbf{J}_1(\lambda_1)) & 0 & \cdots & 0 \\
            0 & f(\mathbf{J}_2(\lambda_2)) & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\mathbf{J}_k(\lambda_k))
        \end{bmatrix}
    \end{equation}
    and \begin{equation}
        f(\mathbf{J}_i(\lambda_i)) = \begin{bmatrix}
            f(\lambda_i) & f'(\lambda_i) & \frac{f''(\lambda_i)}{2!} & \cdots & \frac{f^{(k-1)}(\lambda_i)}{(k-1)!} \\
            0 & f(\lambda_i) & f'(\lambda_i) & \cdots & \frac{f^{(k-2)}(\lambda_i)}{(k-2)!} \\
            \vdots & \vdots & \ddots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\lambda_i) & f'(\lambda_i) \\
            0 & 0 & \cdots & 0 & f(\lambda_i)
        \end{bmatrix}
    \end{equation}
\end{definition}
Obviously, both definitions of matrix functions, based on diagonalization and Jordan canonical form, presuppose that the spectral radius of $\mathbf{A}$, $\rho(\mathbf{A})$, is less than $r$, the radius of convergence.
\subsection{Interpolation-based definition}
Interestingly, in this section we will show that for any $\mathbf{A}\in\mathbb{C}^{n\times n}$ and any sufficiently differentiable $f$, we can find a polynomial $p$ such that $f(\mathbf{A})=p(\mathbf{A})$. First, let us observe from previous sections that only the eigenvalues of $\mathbf{A}$ are actually important for matrix polynomials. Also recall that every matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$ with spectrum $\{\lambda_1,\dots,\lambda_n\}$ has a minimal polynomial $\phi_{\mathbf{A}}$ given by
\begin{equation}\label{eq:minimalpoly}
\phi_{\mathbf{A}}(t):=\prod_{i=1}^{k}(t-\lambda_k)^{n_i}
\end{equation}
which is the unique monic minimal degree ($\text{degr}(\phi_{\mathbf{A}})=n_1+\cdots+n_k\leq n$) polynomial such that $\phi_{\mathbf{A}}(\mathbf{A})=0$.
\begin{theorem}\label{thm:unicity}
Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. Then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})=p_2(\mathbf{A})$ if and only if 
$$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j).$$
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, with spectrum $\{\lambda_1,\ldots,\lambda_k\}$, and two polynomials $p_1$ and $p_2$ such that $p_1(\mathbf{A})=p_2(\mathbf{A})$. Let us define $q$ such that
    \begin{equation*}
        q := p_1 - p_2
    \end{equation*}
    Then, $q(\mathbf{A})=0$, and is thus divisible by $\phi_{\mathbf{A}}$, meaning that
    \begin{equation*}
        \forall j\in\{1,\dots,k\}:\forall i\in\{0,\ldots,n_k-1\}, q(\lambda_j) = 0 \Rightarrow p_1^{(i)}(\lambda_j) = p_1^{(i)}(\lambda_i)
    \end{equation*}
    Similarly, consider two polynomials $p_1$ and $p_2$ such that 
    \begin{equation*}
        \forall j\in\{1,\dots,k\}:\forall i\in\{0,\ldots,n_k-1\},  p_1^{(i)}(\lambda_j) = p_2^{(i)}(\lambda_i)    
    \end{equation*}
    For $i=0$, $q:=p_1-p_2=0$ on the spectrum of $\mathbf{A}$, and is then divisible by $\phi_{\mathbf{A}}$. Then
    \begin{equation*}
        q = K\phi_{\mathbf{A}}
    \end{equation*}
    with $K$ a polynomial. Then, $q(\mathbf{A})=K(\mathbf{A})\phi_{\mathbf{A}}(\mathbf{A}) = 0$ since by definition, $\phi_{\mathbf{A}}(\mathbf{A}) = 0$. And thus, $p_1(\mathbf{A})=p_2(\mathbf{A})$.

    From this reasoning, we conclude that 
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
\end{proof}
In theorem~\ref{thm:unicity}, the conditions involve the evaluation of the polynomials $p_1$ and $p_2$ and their derivatives up to order $n_k-1$ at the eigenvalues of $A$.

However, when the spectrum of $\mathbf{A}$ is simple, each eigenvalue $\lambda_j$ is of multiplicity $n_j=1$. This means that there are no higher order terms corresponding to these eigenvalues in the minimal polynomial, or in other words, there are no repeated roots. Consequently, there is no need to consider the derivatives of the polynomials $p_1$ and $p_2$ because there are no repeated roots for the polynomials to ``match up'' with. Therefore, in this simpler case, we only need to check that the polynomials $p_1$ and $p_2$ agree at the eigenvalues of $\mathbf{A}$. In formal terms, the condition becomes:
\begin{corollary}\label{cor:unicity}
Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. If $\mathbf{A}$ has simple spectrum, then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})=p_2(\mathbf{A})$ if and only if
\begin{equation*}
    \forall j\in\{1,\ldots,k\}: p_1(\lambda_j)=p_2(\lambda_j).
\end{equation*}
\end{corollary}
From this corollary, and from theorem \ref{thm:unicity}, we can confirm our earlier statement : only the spectrum of $\mathbf{A}$ is important for matrix polynomials. More importantly, we observe that $p(\mathbf{A})$ is uniquely defined by its values on the spectrum of $\mathbf{A}$. It seems then natural to extend this definition to any function $f$.
\begin{definition}\label{def:Hermite}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a matrix with minimal polynomial given as in equation \ref{eq:minimalpoly}, and let $f$ be a function that is at least $\max_k\{n_k-1\}$ times differentiable. Say $p$ is its $(n_1,\ldots,n_k)$-\emph{Hermite interpolant} i.e. the polynomial satisfying
    $$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p^{(i)}(\lambda_j)=f^{(i)}(\lambda_j)$$
    of minimal degree. Then we define $f(\mathbf{A})=p(\mathbf{A})$.
\end{definition}
This definition is very powerful as it allows us to manipulate any sufficiently differentiable function $f$ evaluated on $\mathbf{A}$ a polynomial in $\mathbf{A}$. We saw from earlier definitions that polynomials were easily defined in matrices, making this property a robust tool for matrix functions computations. It can however become quite expensive, as it requires the entire knowledge of the spectrum of $\mathbf{A}$. Furthermore, Hermite interpolation is a computationally heavy process, especially when going higher order, and this can be understood when looking at its explicit form:
\begin{equation}
    p(t) = \sum_{i=1}^s\left[\left(\sum_{j=0}^{n_i-1}\frac{1}{j!}\phi_i^{(j)}(\lambda_i)(t-\lambda_i)^j\right)\prod_{j\neq i}(t-\lambda_j)^{n_j}\right]
\end{equation}
This is the so-called Lagrange-Hermite formula. Note the link with minimal polynomial. In conclusion, basing the evaluation of $f(\mathbf{A})$ on definition \ref{def:Hermite} is very flexible, but computationally expensive. Given the matrix functions is not to be evaluated lonely, we will see next strategies to more efficiently evalute $f(\mathbf{A})$.
%%%%%%% NEED TO COMMENT ON THIS %%%%%%%%
\section{The matrix-vector product $f(\mathbf{A})\mathbf{b}$}
\subsection{Introduction}\label{sec:fabintro}
\subsubsection{Motivation}
To motivate the need for a matrix-vector product, we will consider a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$. Let us consider the matrix exponential, such as :
\begin{equation}
    e^{\mathbf{A}} = \sum_{k=0}^\infty \frac{\mathbf{A}^k}{k!}
\end{equation}
Not only does this computation is very heavy (see section \ref{sec:matrixexp} for computation strategies), but it also affects the structure of the matrix. Say for example, we have the following laplacian matrix: 
\begin{equation*}
    \mathbf{A} = \begin{bmatrix}
        2 & -1 & 0 & 0 & \dots &  \dots & 0 \\
        -1 & 2 & -1 & 0 & 0 & \dots & 0 \\
        0 & -1 & 2 & -1 & 0 & \dots & 0 \\
        \vdots & \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
        0 & 0 & \dots & 0 & -1 & 2 & -1 \\
        0 & 0 & \dots & 0 & 0 & -1 & 2
    \end{bmatrix}
\end{equation*}
Obviously, $\mathbf{A}$ is a sparse matrix, and also a rank-structured one : it is a tridiagonal matrix. However, by computing $e^{\mathbf{A}}$, one will get a dense matrix. Storing a dense matrix often becomes challenging as the dimension of the problem grows. Besides storing, computing such a dense matrix is also an issue. However, the matrix-vector product $f(\mathbf{A})\mathbf{b}$ can be stored and computed efficiently.

\subsubsection{Applications}
In numerical linear algebra, numerous methods rely hugely on the matrix-vector product $f(\mathbf{A})\mathbf{b}$.
\paragraph{Power Iteration}
The Power Iteration method is employed to approximate the largest eigenvalue (in absolute value) and its corresponding eigenvector of a given square matrix $\mathbf{A}$. The key operation is a matrix-vector product $\mathbf{A}\mathbf{x}$, iteratively applied. Each iteration takes the form
\begin{equation}
    \mathbf{x}^{(k+1)} = \frac{\mathbf{A}\mathbf{x}^{(k)}}{\|\mathbf{A}\mathbf{x}^{(k)}\|_2}
\end{equation}
The method converges under certain conditions, and its rate of convergence is determined by the ratio of the two largest eigenvalues. In this case, the computational cost is dominated by the matrix-vector product.
\paragraph{Conjugate Gradient Method}

The Conjugate Gradient (CG) method is used for solving linear systems $\mathbf{A}\mathbf{x} = \mathbf{b}$ where $\mathbf{A}$ is positive definite. Each iteration involves a matrix-vector product $\mathbf{A}\mathbf{p}_k$, along with vector additions and scalar multiplications, but not full matrix operations. This allows CG to be used efficiently even when $\mathbf{A}$ is very large but sparse, as it often occurs in discretized optimization problems and PDE-constrained optimization.
\paragraph{Lanczos Algorithm for Tridiagonalization}

The Lanczos algorithm constructs a tridiagonal matrix that is orthogonally similar to the original matrix $\mathbf{A}$. This tridiagonal matrix is often used in eigenvalue problems or singular value decomposition (SVD). Again, the dominant operation here is the matrix-vector product $\mathbf{A}\mathbf{v}_i$ in each iteration.

\paragraph{Krylov Subspace Methods in Solving Linear Systems and Eigenvalue Problems}

More generarly, methods like GMRES, BICGStab, or MINRES operate in a Krylov subspace spanned by ${\mathbf{b}, \mathbf{A}\mathbf{b}, \mathbf{A}^2\mathbf{b}, \ldots}$ for solving linear systems or eigenvalue problems. These methods are particularly efficient when $\mathbf{A}$ is large and sparse, as they also primarily require matrix-vector multiplications.

The key point behind each of these examples is that if matrix-vector products can be computationally cheaper than full matrix-matrix operations, then it could permit sparse, structured, or iterative methods scaling well with problem size.

\subsection{Computationally Efficient Evaluation of $f(\mathbf{A})\mathbf{b}$}
As motivated by \ref{sec:fabintro}, when encountering specific structure in $\mathbf{A}$, such as sparsity, there is a lot to gain if we can store $f(\mathbf{A})\mathbf{b}$. In this section we will work towards a way to evaluate $f(\mathbf{A})\mathbf{b}$ in an intuitive way, thanks to Arnoldi method.

\subsubsection{Formal Definitions}
Firstly, we will define the notion of $\phiAb$, \textit{i.e} the minimal polynomial of $\mathbf{A}$ with respect to the vector $\mathbf{b}$. This is simply the polynomial
\begin{equation}\label{eq:minPolyAb}
\phiAb(t):= \prod_{i=1}^{k}(t-\lambda_i)^{m_i}
\end{equation}
of minimal degree such that $\phiAb(\mathbf{A})\mathbf{b}=\mathbf{0}$. Here $\lambda_1,\ldots,\lambda_k$ are again the eigenvalues of $\mathbf{A}$.

\begin{remark}
    Given a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and a vector $\mathbf{b}\in\mathbb{C}^n$, the minimal polynomial $\phiAb$ is not necessarily the same as the minimal polynomial $\phi_{\mathbf{A}}$ of $\mathbf{A}$.
\end{remark}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and $\mathbf{b}\in\mathbb{C}^n$. Let us consider the following matrix:
    \begin{equation*}
        \mathbf{A} = \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix}
    \end{equation*}
    with the vector
    \begin{equation*}
        \mathbf{b} = \begin{bmatrix}
            1 \\
            0
        \end{bmatrix}
    \end{equation*}
    The eigenvalues of $\mathbf{A}$ are $\lambda_1 = 1, \lambda_2 = 1$, and hence the minimal polynomial $\phi_{\mathbf{A}}(t) = (t-1)^2$.

    Now, let's consider $\phiAb$. For this, we need to find the polynomial of minimal degree such that $\phiAb(\mathbf{A})\mathbf{b} = \mathbf{0}$.

    Observing that $(\mathbf{A} - \mathbf{I})\mathbf{b} = \mathbf{0}$, we find that $\phiAb(t) = t - 1$, which is of minimal degree to satisfy the property.

    Clearly, $\phiAb(t) \neq \phi_{\mathbf{A}}(t)$
\end{proof}

\begin{remark}
    However, it is possible that $\phiAb = \phi_{\mathbf{A}}$.
\end{remark}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and $\mathbf{b}\in\mathbb{C}^n$. Let us consider the following matrix:
    \begin{equation*}
        \mathbf{A} = \begin{bmatrix}
            0 & 1 \\
            0 & 0
        \end{bmatrix}
    \end{equation*}
    With vector
    \begin{equation*}
        \mathbf{b} = \begin{bmatrix}
            1 \\
            1
        \end{bmatrix}
    \end{equation*}
    The eigenvalues of $\mathbf{A}$ are $\lambda_1 = 0, \lambda_2 = 0$, and hence the minimal polynomial $\phi_{\mathbf{A}}(t) = t^2$.

    Now, let's consider $\phiAb$. We find that $(\mathbf{A}^2)\mathbf{b} = \mathbf{0}$ but $(\mathbf{A})\mathbf{b} \neq \mathbf{0}$, which means that the minimal polynomial that satisfies $\phiAb(\mathbf{A})\mathbf{b} = \mathbf{0}$ is $t^2$.

    Thus $\phiAb(t) = \phi_{\mathbf{A}}(t)$.
\end{proof}

\begin{lemma}\label{lem:phiAb}
    Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. Then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})\mathbf{b}=p_2(\mathbf{A})\mathbf{b}$ if and only if 
$$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j).$$
\end{lemma}

\begin{proof}
    From theorem \ref{thm:unicity}, we know that
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
    We also know that 
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \Leftrightarrow p_1(\mathbf{A})\mathbf{b} = p_2(\mathbf{A})\mathbf{b}
    \end{align*}
    Thus, we conclude that
    \begin{align*}
        p_1(\mathbf{A})\mathbf{b} = p_2(\mathbf{A})\mathbf{b} \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
\end{proof}


\begin{theorem}\label{thm:KrylovApprox}
    Let $f$ be a sufficiently differentiable function that has no singularities on the spectrum of a given matrix $\mathbf{A}\in\mathbb{C}^{n \times n}$. Then, with $p$ the unique Hermite interpolating polynomial of $\mathbf{A}$ w.r.t. $\mathbf{b}$ i.e. 
    $$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,m_k-1\}:p^{(i)}(\lambda_j)=f^{(i)}(\lambda_j)$$
    we have that $f(\mathbf{A})\mathbf{b}=p(\mathbf{A})\mathbf{b}$.
\end{theorem}

\begin{proof}
    Let $f$ be a function that is at least $\max_k\{m_k-1\}$ times differentiable, and let $p_1$ be its $(m_1,\ldots,m_k)$-Hermite interpolant. Then, by definition \ref{def:Hermite}, we have that 
    \begin{equation*}
        f(\mathbf{A}) = p_1(\mathbf{A})
    \end{equation*}
    Then, by lemma \ref{lem:phiAb}, let us define $p_2$ such that
    \begin{equation*}
        p_2(\mathbf{A})\mathbf{b} = p_1(\mathbf{A})\mathbf{b}
    \end{equation*}
    Then, we have that
    \begin{equation*}
        p_1(\mathbf{A})\mathbf{b} = p_2(\mathbf{A})\mathbf{b} = f(\mathbf{A})\mathbf{b}
    \end{equation*}
\end{proof}

\subsubsection{The Arnoldi Method}
The Arnoldi method is a reduction method that allows low-rank approximation of a given matrix $\mathbf{A}$. It is based on the Hessenberg reduction of a matrix $\mathbf{A}$. More formally
\begin{definition}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, the Arnoldi method approximates its Hessenberg reduction given by
    \begin{equation}
        \mathbf{V}^*\mathbf{A}\mathbf{V} = \mathbf{H}
    \end{equation}
    where $\mathbf{V}\in\mathbb{C}^{n\times n}$ is unitary, and $\mathbf{H}\in\mathbb{C}^{n\times n}$ is upper Hessenberg. As it is a (low-rank) approximation, Arnoldi will compute the following decomposition
    \begin{equation}
        \mathbf{V}_k^*\mathbf{A}\mathbf{V}_k = \mathbf{H}_k
    \end{equation}
    with $\mathbf{V}_k\in\mathbb{C}^{n\times k}$ unitary, and $\mathbf{H}_k\in\mathbb{C}^{k\times k}$ upper Hessenberg. That means that $\mathbf{H}_k$ is the orthogonal projection of $\mathbf{A}$ onto the $k$th Krylov subspace $\mathcal{K}_k(\mathbf{A},\mathbf{v_1})$, with $\mathbf{v_1}$ the first column of $\mathbf{V}$.
\end{definition}
The Arnoldi method is an iterative method, following this algorithm:
\begin{algorithm2e}
    \SetAlgoLined
    \KwData{$A \in \mathbb{C}^{n \times n}$, $v_1 \in \mathbb{C}^n$ a unit vector in the chosen norm (line 7)}
    \KwResult{$H_n \in \mathbb{C}^{n\times n}$}
    \caption{Arnoldi Iteration (Modified Gram-Schmidt)}
    \For{$k = 1 \KwTo n$}{
        $\mathbf{w} \gets \mathbf{A}\mathbf{v}_k$\;
        \For{$i = 1 \KwTo k$}{
            $h_{ik} \gets \mathbf{v}_i^*\mathbf{w}$\;
            $\mathbf{w} \gets \mathbf{w} - h_{ik}\mathbf{v}_i$\;
        }
        $h_{k+1,k} \gets \|\mathbf{w}\|$\;
        \If{$h_{k+1,k} = 0$}{
            \textbf{break}\;
        }
        $\mathbf{v_{k+1}} \gets \mathbf{w}/h_{k+1,k}$
    }
    %\label{alg:arnoldi}
    \label{alg:arnoldi}
\end{algorithm2e}

Here the orthogonalization process (lines 3-6) is a modified Gram-Schmidt process. Also, when implementing, we will consider an $\ell_2$ norm. Numerical conditions can imply loss of orthogonality. To avoid this, it can be necessary to reorthogonalize, \textit{i.e.} to reapply the Gram-Schmidt process. In the following we don't note any particular loss of orthogonality, and thus we sticked with Modified Gram-Schmidt. Note though that in other methods, such as Lanczos Tridiagonalization, reorthogonalization is an absolute necessity.
\subsubsection{The matrix-vector product $f(\mathbf{A})\mathbf{b}$}
Recall the problem is to approximate the matrix-vector product $f(\mathbf{A})\mathbf{b}$. Now we have all the tools to achieve that efficiently.  
\begin{lemma}\label{lem:arnoldi1}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ and $\mathbf{b}\in\mathbb{C}^n$. Then, the matrix vector product $f(\mathbf{A})\mathbf{b}$ can be approximated by
    \begin{equation}
        f(\mathbf{A})\mathbf{b} \approx \|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{e}_1
    \end{equation}
    Where $\mathbf{V}_k$ and $\mathbf{H}_k$ are the matrices computed by the Arnoldi method after $k$ iterations. $\mathbf{H}_k$ is upper Hessenberg and $\spn{\left(\mathbf{V}_k\right)}=\mathcal{K}_k(\mathbf{A},\mathbf{b}) = \spn{\left(\mathbf{b}, \mathbf{A}\mathbf{b}, \mathbf{A}^2\mathbf{b}, \dots, \mathbf{A}^{k-1}\mathbf{b}\right)}$. $\mathbf{e}_1$ is the first column of the identity matrix.
\end{lemma}
\begin{proof}
    Consider we want to approximate the matrix-vactor product $f(\mathbf{A})\mathbf{b}$. We will use the Arnoldi method (algorithm 1). For initial unit vector we choose $\mathbf{v}_1 = \mathbf{b}/\|\mathbf{b}\|_2$ which is indeed an $\ell_2$ unit vector. According to the Hessenberg reduction of $\mathbf{A}$, we have that
    \begin{equation*}
        f(\mathbf{A})\mathbf{b} \approx f(\mathbf{V_k}\mathbf{H}_k\mathbf{V}_k^*)\mathbf{b}
    \end{equation*}
    Since $\mathbf{V}_k$ is unitary, we have
    \begin{equation*}
        f(\mathbf{A})\mathbf{b} \approx \mathbf{V}_k f(\mathbf{H}_k)\mathbf{V}_k^*\mathbf{b}
    \end{equation*}
    Furthermore, as we chose $\mathbf{v}_1 = \mathbf{b}/\|\mathbf{b}\|_2$, we have that
    \begin{equation*}
        \mathbf{b} = \|\mathbf{b}\|_2\mathbf{v}_1 = \|\mathbf{b}\|_2\mathbf{V}_k\mathbf{e}_1
    \end{equation*}
    Thus, we have that
    \begin{equation*}
        f(\mathbf{A})\mathbf{b} \approx \|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{V}_k^*\mathbf{V}_k\mathbf{e}_1 = \|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{e}_1
    \end{equation*}
\end{proof}

\begin{lemma}\label{lem:arnoldi2}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ and let $\mathbf{V}_k$, $\mathbf{H}_k$ be the matrices computed by the Arnoldi method after $k$ iterations. Then, for any polynomial $p_j$ of degree $j\leq k-1$, we have that
    \begin{equation}
        p_j(\mathbf{A})\mathbf{b} = \mathbf{V}_kp_j(\mathbf{H}_k)\mathbf{e}_1
    \end{equation}
\end{lemma}

\begin{lemma}\label{lem:arnoldi3}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ and let $\mathbf{V}_k$, $\mathbf{H}_k$ be the matrices computed by the Arnoldi method after $k$ iterations. Then, given $p_{k-1}$ the unique Hermite interpolant of $\mathbf{A}$ w.r.t. $\mathbf{b}$ of degree $k-1$, we have that
    \begin{equation}
        \|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{e}_1 = p_{k-1}(\mathbf{A})\mathbf{b}
    \end{equation}
\end{lemma}
\begin{proof}
    Since $p_{k-1}$ is the unique Hermite interpolant of $\mathbf{A}$ w.r.t. $\mathbf{b}$ of degree $k-1$, we have that
    \begin{equation*}
        \|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{e}_1 = \|\mathbf{b}\|_2\mathbf{V}_k p_{k-1}(\mathbf{H}_k)\mathbf{e}_1
    \end{equation*}
    And since 
    \begin{equation*}
        \|\mathbf{b}\|_2\mathbf{V}_k p_{k-1}(\mathbf{H}_k)\mathbf{e}_1 = \|\mathbf{b}\|_2p_{k-1}(\mathbf{A})\mathbf{q}_1 = p_{k-1}(\mathbf{A})\mathbf{b}
    \end{equation*}
    We indeed have
    \begin{equation*}
        \|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{e}_1 = p_{k-1}(\mathbf{A})\mathbf{b}
    \end{equation*}
\end{proof}

\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ and let $m = \deg(\phiAb)$ then
    \begin{equation}
        f(\mathbf{A})\mathbf{b} = \|\mathbf{b}\|_2\mathbf{V}_m f(\mathbf{H}_m)\mathbf{e}_1
    \end{equation}
\end{theorem}
\begin{proof}
    Following theorem \ref{thm:KrylovApprox}, we have that
    \begin{equation*}
        f(\mathbf{A})\mathbf{b} = p_{m-1}(\mathbf{A})\mathbf{b}
    \end{equation*}
    With $p_{m-1}$ the unique Hermite interpolant of $\mathbf{A}$ w.r.t. $\mathbf{b}$ of degree $m-1$ with $\deg(\phiAb)= m$. Then, by lemma \ref{lem:arnoldi1}, \ref{lem:arnoldi2} and \ref{lem:arnoldi3}, we have that
    \begin{equation*}
        f(\mathbf{A})\mathbf{b} = \|\mathbf{b}\|_2\mathbf{V}_m f(\mathbf{H}_m)\mathbf{e}_1
    \end{equation*}
\end{proof}
\section{Algorithms}
In this section, we will cover two main algorithms for the evalutation of matrix functions.
\subsection{Dense case}
First, let us consider the computation of $f(\mathbf{A})$. Meaning we put aside the matrix-vector product (for now). More specifically, our approach will be to use definition \ref{def:Hermite} to compute $f(\mathbf{A})$.
\subsubsection{Dependencies}
We are provided with a function \texttt{hess\_and\_phi()} that computes the Hessenberg reduction of a given matrix and its associated minimal polynomial. It takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$. It then follows these simple steps:
\begin{enumerate}
    \item Compute the Hessenberg reduction of $\mathbf{A}$, i.e. $\mathbf{V}^*\mathbf{A}\mathbf{V} = \mathbf{H}$
    \item It computes its Jordan Canonical Form from $\mathbf{H}$
    \item It computes the minimal polynomial of $\mathbf{A}$, $\phi_{\mathbf{A}}$ through its Jordan Canonical Form
    \item It returns $\mathbf{V}$, $\mathbf{H}$, $\mathbf{J}$, $\lambda_j$ (the eigenvalues of $\mathbf{A}$) and $n_i$ (the multiplicity of $\lambda_i$ in $\phi_{\mathbf{A}}$) 
\end{enumerate}
Note that step 2 makes perfect sense, referring the following theorem:
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ with a Hessenberg reduction defined by $\mathbf{V}^*\mathbf{A}\mathbf{V} = \mathbf{H}$. Then $\mathbf{A}$ and $\mathbf{H}$ share the same Jordan Canonical Form.
\end{theorem}
\begin{proof}
    $\mathbf{V}$ is unitary, hence $\mathbf{A}$ and $\mathbf{H}$ are similar matrices, meaning they have the same eigenvalues. Thus, they share the same Jordan Canonical Form.
\end{proof}
To recover the minimal polynomial of $\mathbf{A}$, one simply retrives the $\lambda_j$ and the $n_i$ from the previously described steps, and then compute the polynomial this way:
\begin{equation*}
    \phi_{\mathbf{A}}(t) = \prod_{i=1}^{k}(t-\lambda_k)^{n_i}
\end{equation*}
In the supplementary materials, you will find an implementation of this, called\\ \texttt{construct\_minimal\_polynomial()}. One quick way to assess if the minimal polynomial is constructed correctly, is to evaluate it at $\mathbf{A}$. Taking the $\ell_2$ norm of the result should be close to zero. Here, with \texttt{test1.mat} (a 5 by 5 matrix), we obtain $\phi_{\mathbf{A}}(\mathbf{A}) = 3.3523e-12$. This is indeed numerically close to zero.

For the Hermite interpolation, several options are possible. First, a routine \texttt{hermite\_interp()} is provided in the supplementary materials, it computed the Hermite interpolation by divided differences. However, I had more precise results with an alternative method proposed by Or Werner, BGU, Israel, based on the Hermite method. In the following, I used the latter approach, the routine is provided in the supplementary material aswell : \texttt{HermitePoly()}.
\subsubsection{Implementation}
From the previous elements, we can construct a routine called \texttt{matrix\_function()} that takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and a function $f$ (Algorithm 2)

\begin{algorithm2e}
    \SetAlgoLined
    \KwData{$\mathbf{A} \in \mathbb{C}^{n \times n}$, $f$ a function}
    \KwResult{$f(\mathbf{A})$}
    \caption{Matrix Function}
    $\lambda_j, n_i \gets$ \texttt{hess\_and\_phi($\mathbf{A}$)}\;
    $\texttt{FdF} \gets [\lambda_j;f(\lambda_j);f'(\lambda_j);\dots]$ $\forall j$\;
    $P \gets \texttt{hermitePoly(FdF)}$\;
    $f(\mathbf{A}) \gets P(\mathbf{A})$

\end{algorithm2e}

Note that line 2 of Alorithm 2 is specific to the function \texttt{hermitePoly}. This function was made outside of the scope of this note, and is provided in the supplementary materials. $n_i$ is the highest useful differentiation level, and is actually computed from the output of \texttt{hess\_and\_phi} by $\max(m)$ where $m$ is an array containing the multiplicity of every eigenvalues of $\mathbf{A}$.

The routine is provided in the supplementary materials.
\subsubsection{Results}
In this part, we will compare the performance of our routine \texttt{matrix\_function()} with the built-in \texttt{funm()} function from \texttt{Matlab}. The built-in Matlab function uses a Schur-Parlett algorithm from \cite{davies2003schur}. To compare both methods, we generate three types of matrices of different sizes. More precisely, we will investigate both algorithm's performances on symmetric, diagonal and dense matrices, on the following functions: cosine, sine and exponential.
\begin{figure}
    \begin{subfigure}[b]{0.3\textwidth}
        \input{figures/comp_sine}
        \caption{}
        \label{fig:comp_sine}
    \end{subfigure}\hspace{.055\linewidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \input{figures/comp_cosine}
        \caption{}
        \label{fig:comp_cosine}
    \end{subfigure}\hspace{.025\linewidth}
    \begin{subfigure}[b]{0.3\textwidth}
        \input{figures/comp_exp}
        \caption{}
        \label{fig:comp_exp}
    \end{subfigure}
    \caption{Measure of performance of the algorithm to compute $f(\mathbf{A})$ according to definition \ref{def:Hermite}. We assume that \cite{davies2003schur} (Schur Parlett) algorithm is correct, and we measure the relative error. We compare the performance of our algorithm on several types of matrices : diagonal (- -), symmetric (-*) and dense (--). Matrix coefficients are randomly filled such that for all non-zero entry: $a_{ij}\sim\mathcal{U}(0,1)$, this ensures $\mathbf{A}$ is full rank. We test it on three different functions : Sine (a), Cosine (b) and Exponenial (c). We note that starting from a certain matrix size, the algorithm lose considerably in performance.}
    \label{fig:comp}
\end{figure}

Figure \ref{fig:comp} illustrates the result of the measurement of performance of the algorithm. The relative error between \cite{davies2003schur} and our results is computed:
\begin{equation*}
    \text{relative error} = \frac{\|f(\mathbf{A})-\texttt{funm}(\mathbf{A},f)\|_2}{\|\texttt{funm}(\mathbf{A},f\|_2}
\end{equation*}
We note that for small matrices ($n\leq 10$), the hermite interpolation approach is precise and has residual error inferior to $10^{-11}$, which is somewhat close to numerical precision (around $10^{-16}$ as we work with double precision). However, starting from a certain matrix size, the algorithm loses considerably in performance: both in precision and in computation time. Interestingly, this behavior is independant to the matrix structure : diagonal, symmetric or dense. It also seem to be independant to the function we are evaluating (though we have slightly lower relative error for the exponential function as depicted in figure \ref{fig:comp_exp}).

We conclude that this algorithm, while being elegant, is quickly inefficient, and lead to both numerical and computational issues, even at small matrix sizes.

\subsection{Matrix-vector product}
\subsubsection{Implementation}
In this section, we will implement the matrix-vector product $f(\mathbf{A})\mathbf{b}$, as described in section \ref{sec:fabintro}. We will use the Arnoldi method to achieve this. As the theory has been well described in section \ref{sec:fabintro}, the implementation is straight-forward :
\begin{algorithm2e}
    \SetAlgoLined
    \KwData{$\mathbf{A} \in \mathbb{C}^{n \times n}$, $\mathbf{b} \in \mathbb{C}^n$, $f$ a function and $k\in\mathbb{N}$ the dimension of the Krylov subspace}
    \KwResult{$f(\mathbf{A})\mathbf{b}$}
    \caption{Matrix-Vector Product}
    $\mathbf{v}_1 \gets \mathbf{b}/\|\mathbf{b}\|_2$\;
    $[\mathbf{V},\mathbf{H}] \gets$ \texttt{arnoldi}($\mathbf{A},\mathbf{v}_1,k$)\;
    $e_1 \gets$ first column of the identity matrix\;
    $f \gets \|b\|_2\mathbf{V}f(\mathbf{H})\mathbf{e}_1$\;
\end{algorithm2e}

Where \texttt{arnoldi()} is the modified Gram-Schmidt Arnoldi iteration described in algorithm 1. This routine is provided in the supplementary materials.

\subsubsection{Results}
To evaluate the performance of this Matrix-vector product routine, we will test it versus the naive way of doing it in \texttt{Matlab}, meaning computing $f(\mathbf{A})$ explicitely, then multiplying it by $\mathbf{b}$. In our first experiment (figure \ref{fig:comp_fab}), we are given a matrix $\mathbf{A}$ that is a large sparse matrix (\texttt{test2.mat}). More specifically it is a graded L-shape pattern from \cite{george1978automatic}. The vector $\mathbf{b}$ is filled with uniformly distributed random coefficients such that $b_i\sim\mathcal{U}(0,1)$. The figure \ref{fig:comp_fab_assignement} shows that for such a setup, the proposed algorithm reaches a relative error equals to numerical precision few iterations afters 20. This means 
\begin{equation*}
    \frac{\|\|\mathbf{b}\|_2\mathbf{V}_k f(\mathbf{H}_k)\mathbf{e}_1 - f(\mathbf{A})\mathbf{b}\|_2}{\|f(\mathbf{A})\mathbf{b}\|_2} = \epsilon
\end{equation*}
for $k > 20$, and where $\epsilon$ is the machine precision. Note that this result is very specific to the problem depicted in figure \ref{fig:comp_fab}, and the threshold for $k$ will differ in function of the considered problem. Another interpretation is that computing $f(\mathbf{A})\mathbf{b}$ on the smaller Krylov subspace $\mathcal{K}_k(\mathbf{A},\mathbf{b})$ gives extremely good results, even when $k << n$. 
\begin{figure}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/comp_fab_pattern.tex}
        \caption{}
        \label{fig:comp_fab_pattern}
    \end{subfigure}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/comp_fab_assignement.tex}
        \caption{}
        \label{fig:comp_fab_assignement}
    \end{subfigure}
    \caption{Evaluation of the performance of the matrix-vector product routine. We compare the performance of our method against the naive way of doing it in \texttt{Matlab}. We test it on three different functions : Sine (- -), Cosine (-*) and Exponenial (--) on figure (b). Those functions are applied to a large matrix $\mathbf{A}$ with sparisity pattern given in (a). Our method allows for precise approximation of $f(\mathbf{A})b$ even when working on a very low rank Hessenberg reduction (b). The vector $\mathbf{b}$ is a random vector whose coefficients are uniformly distributed such that $b_i\sim\mathcal{U}(0,1)$.}
    \label{fig:comp_fab}
\end{figure}

Obviously, this very interesting result lets us think that there is a lot of potential computational gain in evaluating $f(\mathbf{A})\mathbf{b}$ on this smaller Krylov subspace. The computational gain for the problem solved in figure \ref{fig:comp_fab} is described in Table \ref{tab:fab}. We note that for this problem, our method considerably outperforms the naive approach of evaluating $f(\mathbf{A})$ seperately, regardless of the function (though the biggest gain seem to come from the matrix exponential). 

However, this does not give us any insight about how those two algorithms sacle up or down. To investigate this, we will use the BCSPWR matrix collection (\cite{boisvert1997matrix}). This collection contains matrices of different sizes, and different sparsity patterns. We will test our method against the naive approach on this collection. The results are depicted in Table \ref{tab:fab2}. We notice that for very low dimension ($n < 300$) the naive approach slightly outperforms ours for trigonometric functions. However, for larger matrices, and for the exponential function, working in the Krylov subspace is the way to go. For the largest matrix, where $n=5300$, the naive approach takes half a minute, while our method takes less than a tenth of a second. This is a considerable gain in performance, and could be crucial in some applications. As this margin increases with the matrix size, we can expect our method to be even more efficient for larger matrices. We note the the optimal $k$ is not evolving throughout the matrix dataset, this is most likely due to the fact that given they represent similar data, they must have similar condition numbers.

\begin{table}
    \centering
    \caption{Comparison of computational performance of both methods for evaluating $f(\mathbf{A}\mathbf{b})$ with $\mathbf{A}$ the matrix in \ref{fig:comp_fab_pattern}. Performance was measured in ms. Optimal $k$ was determined when the relative error went below $1e-14$, as we worked on double precision. Tests were run on an Intel Core i7-1185G7 @ 3.00GHz with 16GB RAM. We note that for this problem, our method is outperforming by a significant margin the naive approach.}
    \begin{tabular}{|c|c|c|c|}
        \hline
        \textbf{fun} & \textbf{Naive way (ms)} & \textbf{Krylov method (ms)} & \textbf{Optimal $k$} \\\hline
        \texttt{exp()} & 151.2 & 6.3 & 21 \\ \hline
        \texttt{cos()} & 121.8 & 11.2 & 23 \\ \hline
        \texttt{sin()} & 112.6 & 8.1 & 23 \\
        \hline
    \end{tabular}
    \label{tab:fab}
\end{table}

\begin{table}
    \centering
    \caption{Comparison of the naive approach and the Krylov approach on BCSPWR matrix collection. Performance was measured in ms. Optimal $k$ was determined when the relative error went below $1e-14$.}
    \begin{tabular}{|c|c||c|c|c|c|c|c|}
        \hline
        Matrix & $n$ & \multicolumn{2}{|c|}{\texttt{exp()}} & \multicolumn{2}{|c|}{\texttt{cos()}} & \multicolumn{2}{|c|}{\texttt{sin()}} \\\hline
        & & Naive & Krylov & Naive & Krylov & Naive & Krylov \\\hline
        BCSPWR01 & 39 & \textbf{1.0} & 1.3 & \textbf{0.55} & 1.5 & \textbf{0.38} & 5.3\\\hline
        BCSPWR02 & 49 & 3.2 & \textbf{1.9} & \textbf{0.66} & 3.7 & \textbf{0.48} & 1.6\\\hline
        BCSPWR03 & 118 & 16.6 & \textbf{2.3} & \textbf{1.3} & 3.0 & \textbf{0.87} & 3.1\\\hline
        BCSPWR04 & 274 & 6.5 & \textbf{1.6} & \textbf{5.2} & 8.7 & \textbf{5.9} & 6.4\\\hline
        BCSPWR05 & 443 & 26.4 & \textbf{2.7} & 21.8 & \textbf{4.6} & 21.5 & \textbf{3.6}\\\hline
        BCSPWR06 & 1454 & 436.6 & \textbf{11.9} & 299.6 & \textbf{10.3} & 399.6 & \textbf{12.1}\\\hline
        BCSPWR07 & 1612 & 484.0 & \textbf{11.9} & 392.0 & \textbf{14.7} & 393.2 & \textbf{14.7}\\\hline
        BCSPWR08 & 1624 & 504.4 & \textbf{13.4} & 414.5 & \textbf{12.4} & 433.8 & \textbf{14.6}\\\hline
        BCSPWR09 & 1723 & 628.1 & \textbf{11.7} & 593.7 & \textbf{10.3} & 531.8 & \textbf{10.1}\\\hline
        BCSPWR10 & 5300 & 32868 & \textbf{98.4} & 27447 & \textbf{96.7} & 23948 & \textbf{89.7}\\\hline
    \end{tabular}
    \label{tab:fab2}
\end{table}

\section{Applications}
In this section, we will try to apply the previously mentionned algorithm to more practical problems. We will see how to take advantage of problem's nature thanks to those tools in order to solve them with more efficiency.
\subsection{Matrix Exponential}\label{sec:matrixexp}
\subsubsection{Context}
Consider the simple system of ODEs
\begin{equation}\label{eq:ode}
    \frac{d\mathbf{x}}{dt} = \mathbf{A}\mathbf{x}
\end{equation}
with initial condition $\mathbf{x}(0) = \mathbf{x}_0\in\mathbb{R}^n$. Then we know the solution to be given by $\mathbf{x}(t)=e^{\mathbf{A}t}\mathbf{x}_0$. However, for all but the stablest systems, this is not a good method, due to issues such as stability and stiffness. Here we consider for instance the 2D convection-diffusion equation for the flow $\mathbf{u}(x,y)$:
\begin{equation}\label{eq:convectiondiffusion}
    \frac{d \mathbf{u}}{d t}=\epsilon\Delta\mathbf{u}+\alpha\cdot\nabla\mathbf{u}
\end{equation}
with Dirichlet boundary conditions and $\epsilon\in\mathbb{R}^{+}_0$ and $\alpha\in\mathbb{R}^2$. Simple time-stepping methods are known to be unstable at large time-steps, and our exponential scheme suffers from similar problems, $\textit{i.e}$ the time-step $t$ cannot be taken too large. However, it remains an interesting subject to evaluate the impact of using appropriate routines to compute $\mathbf{x}(t)$.

The convection-diffusion equation (equation \ref{eq:convectiondiffusion}) is split in two parts. First, a diffusion term $\epsilon\Delta\mathbf{u}$, and a convection term $\alpha\cdot\nabla\mathbf{u}$. The variable $\epsilon$ is the diffusivity and $\alpha$ the velocity. The ODE is formed by discretizing the 2D convection-diffusion equation using a finite difference scheme. The discrete Laplacian and gradient operators (L and D in the routine) represent diffusion and convection, respectively. The domain is discretized using a uniform grid with finite difference methods. For the Laplacian, a central difference is used, and for the gradient, a forward difference is used. The matrix $\mathbf{A}$ is then formed by combining the two discretized operators, and the solution is formed :
\begin{equation}\label{eq:odesolution}
    \mathbf{u}(t) = e^{\mathbf{A}t}\mathbf{u}_0
\end{equation}
Note that equation \ref{eq:convectiondiffusion} is a simple case of convection-diffusion where it is assumed that $\epsilon$ is constant, and that there are no sources or sinks (else it would make the equation slightly more complex). 

We notice that the solution (equation \ref{eq:odesolution}) is a simple matrix-vector product $f(\mathbf{A})\mathbf{b}$ where $f() := \exp()$ and $\mathbf{b} = \mathbf{u}_0$. We will use the matrix-vector product routine described in section \ref{sec:fabintro} to compute this solution. We will compare it to the naive approach of computing $f(\mathbf{A})$ explicitely, then multiplying it by $\mathbf{u}_0$.
\subsubsection{Results}

We observe that once again, working on a much smaller Krylov subspace give machine-precision approximation (figure \ref{fig:ode_comp}). The optimal $k$ found was 27, which should allow for a big speed-up in solving this ODE. Indeed, the naive approach takes 13.54 seconds to compute the solution, whereas when working on this smaller Krylov subspace, for $k$ optimally chosen, it only took 225 milliseconds, this is almost a 60 times speed-up.

\begin{figure}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ode_conv.tex}
        \caption{}
        \label{fig:ode_conv}
    \end{subfigure}\hspace{.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ode_sparse.tex}
        \caption{}
        \label{fig:ose_sparse}
    \end{subfigure}
    \caption{Convergence of our algorithm for computing $f(\mathbf{A})\mathbf{b}$ for solving the ODE (equation \ref{eq:odesolution}). We note that on our setup, $\mathbf{A}\in\mathbb{C}^{2500\times 2500}$. Machine precision is quickly reached, at only dimension 27 (a). The matrix $\mathbf{A}$ is strongly structured (b).}
    \label{fig:ode_comp}
\end{figure}

Interestingly, we note that the choice of parameters in the ODE is having a big impact on the performance of the algorithm. If we vary the diffusivity $\epsilon$, we observe that the convergence is considerably slowed down (figure \ref{fig:ode_eps}). The algorithm remains quicker than the naive approach, though it seems important to bear in mind that minor perturbations to some of the problem's components (figures \ref{fig:ode_0.05} and \ref{fig:ode_0.1}) can lead to a considerable change in the convergence behavior. This kind of behavior is usually the definition of a non robust method. More advanced techniques to solve ODEs exist, such as upwind scheme (\cite{lewis2004fundamentals}), or more generally higher order finite-difference schemes and Strong Stability Runge-Kuttas. We will not discuss them here, but it is important to keep in mind that this method is not robust to perturbations in the problem's parameters.

\begin{figure}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ode_0.05.tex}
        \caption{$\epsilon = 0.05$}
        \label{fig:ode_0.05}
    \end{subfigure}\hspace{.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ode_0.1.tex}
        \caption{$\epsilon = 0.1$}
        \label{fig:ode_0.1}
    \end{subfigure}
    \caption{Convergence of our algorithm for different diffusivity value $\epsilon$. We note that as the dissufivity increases, the need to work in higher dimension increases too, thus slowing down the convergence of our algorithm. This suggests that diffusivity impacts the conditioning of $\mathbf{A}$.}
    \label{fig:ode_eps}
\end{figure}

\begin{figure}
    \centering
    \includegraphics[width=.7\linewidth]{figures/convection_krylov.pdf}
    \caption{Result of solving the convection-diffusion equation with our method. For the sake of conciseness, we are not showing results from the exact solution as they are humanly undistinguishable.}
\end{figure}

\subsection{The sign function}\label{sec:sign}
\subsubsection{Background and theory}
In control theory we are often interested in the eigenvalues $\lambda$ of system matrices with $Re(\lambda)>0$, since they correspond to unstable poles. In the design of controllers it is therefore interesting to have an efficient way to count the number of eigenvalues of a matrix in the right half-plane $Re(z)>0$. Here we will build such a method.
\begin{theorem}\label{thm:sign}
Let $\mathbf{A}\in\mathbb{C}^{n \times n}$ be a matrix with $k_{-}$ eigenvalues in the left plane, $k_{+}$ eigenvalues in the right plane and none on the imaginary axis, counting multiplicity. Let $\text{sgn}:\mathbb{C}\mapsto \{1,-1\}$ be defined by
$$\text{sgn}(z)= \begin{cases} 
      1 & Re(z)\geq 0 \\
      -1 & Re(z)<0. 
   \end{cases}
$$
Then $\text{trace}(\text{sgn}(\mathbf{A}))=k_{+}-k_{-}$.
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$. To stay in a very general scenario, and consider cases where $\mathbf{A}$, let us consider its Jordan Canonical Form :
    \begin{equation*}
        \mathbf{A} = \mathbf{VJV}^{-1}
    \end{equation*} 
    Recall that by theorem \ref{thm:jordan}, consider $f$ a function, then 
    \begin{equation*}
        f(\mathbf{A}) = \mathbf{V}f(\mathbf{J})\mathbf{V}^{-1}
    \end{equation*}
    Thus let us consider the decomposition where 
    \begin{equation*}
        \mathbf{J} = \begin{pmatrix}
            \mathbf{J}_{k_+} & 0 \\ 0 & \mathbf{J}_{k_-}
        \end{pmatrix}
    \end{equation*}
    such that $\mathbf{J}_{k_+}$ has $k_+$ eigenvalues in the right plane, and $\mathbf{J}_{k_-}$ has $k_-$ eigenvalues in the left plane. Then, we have that
    \begin{equation*}
        \text{sgn}(\mathbf{J}) = \begin{pmatrix}
            \mathbf{I}_{k_+} & 0 \\ 0 & -\mathbf{I}_{k_-}
        \end{pmatrix}
    \end{equation*}
    where $\mathbf{I}_n$ is the identity matrix of size $n$. Then, we have that
    \begin{equation*}
        \text{trace}(\text{sgn}(\mathbf{J})) = k_+ - k_-
    \end{equation*}
    And thus, we have that
    \begin{equation*}
        \text{trace}(\text{sgn}(\mathbf{A})) = \text{trace}(\text{sgn}(\mathbf{VJV}^{-1})) = \mathbf{V}\text{trace}(\text{sgn}(\mathbf{J}))\mathbf{V}^{-1} = (k_+-k_-)\mathbf{V}\mathbf{V}^{-1} = k_+ - k_-
    \end{equation*}
\end{proof}
\subsubsection{Stability of a system}
In this section, we will see how theorem \ref{thm:sign} is a powerful tool for system stability assessment. Let us consider a system modeled by the matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, large and sparse. Say it has $p$ distinct eigenvalues $\lambda_i$, $i=1,\dots,p$. We know that if $\forall i, Re(\lambda_i)<0$, then the system is stable. Using the sign function, it immediately rewrites, if $\forall i, sign(Re(\lambda_i)) = -1$ then the system is stable. From theorem \ref{thm:sign}, we can design an algorithm that counts positive eigenvalues of a system. Obviously, when $n$ is extremely large, computing eigenvalues directly on $\mathbf{A}$ is not an option. Algorithm 4 takes advantage of the properties of Arnoldi method and estimates via Monte-Carlo sampling the number of positive eigenvalues of $\mathbf{A}$.

\begin{algorithm2e}
    \SetAlgoLined
    \KwData{$A \in \mathbb{C}^{n \times n}$, $k \in \mathbb{N}$, $N \in \mathbb{N}$}
    \KwResult{$k_+ \in \mathbb{R}$}
    \caption{Computing $k_{+}(A)$}
    $\mathbf{q} \gets \mathbf{0}\in\mathbb{R}^{N}$\;
    \For{$i = 1$ \KwTo $N$}{
      $\mathbf{u} \gets \text{randn}(n, 1)$\;
      $\hat{\mathbf{u}} \gets \frac{\mathbf{u}}{\|\mathbf{u}\|}$\;
      $[H, Q] \gets \text{arnoldi}(A, \hat{\mathbf{u}}, k)$\;
      $q(i) \gets \text{trace}(\text{sign}(H)))$\;
    }
    $q_+ := \text{mode}(\mathbf{q}) $\;
    $k_+\gets (q+k)/2$
\end{algorithm2e}
\subsubsection{Limitation of simple Arnoldi Method}
In the previous algorithm to compute positive eigenvalues, we assume the function \texttt{arnoldi()} is the Matlab's translation of Algorithm 1, \textit{i.e} Modified Gram-Schmidt Arnoldi method. In this section we will see that the convergence of Arnoldi Method is actually not guaranteed in a reasonable time for all systems. The efficiency of the Arnoldi algorithm is highly correlated to the condition number of the matrix $\mathbf{A}$. We know from experience, that Ritz Value struggle to converge properly where eigenvalues of the matrix are very close to each other.

Provided with this note, a matrix that has those properties. Using Matlab's \texttt{eigs} function, we observe that the eigenvalues of $\mathbf{A}$ are clustered and very close to each other (figure \ref{fig:ritz_poor_cond}). This leads ultimately to very slow convergence of the eigenvalues using Arnolid method (figure \ref{fig:sign_difficult_est}). 
\begin{figure}
    \centering
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ritz_poor_cond.tex}
        \caption{}
        \label{fig:ritz_poor_cond}
    \end{subfigure}\hspace{0.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ritz_good_cond.tex}
        \caption{}
        \label{fig:ritz_good_cond}
    \end{subfigure}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/sign_difficult_est.tex}
        \caption{}
        \label{fig:sign_difficult_est}
    \end{subfigure}\hspace{0.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/sign_easy_est.tex}
        \caption{}
        \label{fig:sign_easy_est}
    \end{subfigure}
    \caption{Comparison of the convergence of Arnoldi method to two different matrices. The first matrix (figures \ref{fig:ritz_poor_cond} and \ref{fig:sign_difficult_est}) is from section \ref{sec:sign}. We see from figure \ref{fig:ritz_poor_cond} that the eigenvalues are clustered and very close to each other, leading to poor convergences of the Ritz values. We see that even after 200 iterations, the method is far from having converged. On the other end of the spectrum, when applied to the matrix we have studied earlier in the matrix-vector product, which is better conditioned (figures \ref{fig:ritz_good_cond} and \ref{fig:sign_easy_est}), here, the eigenvalues are well spread, and the Ritz values converge quickly. We see that after 200 iterations, the Ritz values perfectly match the eigenvalues. Figures \ref{fig:sign_difficult_est} and \ref{fig:sign_easy_est} show the relative error in the estimation of the eigenvalues. And indeed we see that for the first matrix, the relative error is still very high after 200 iterations, whereas for the second matrix, the relative error is equal to machine precision from 150 iterations.}
    \label{fig:fullritz}
\end{figure}

From this observation, we can easily say that with our previous implementation of Arnoldi, it seems unsafe to use Algorithm 4 as a tool to estimate the number of positive eigenvalues. We need to find a way to replace line 5 of Algorithm 4 by a more robust method.

\subsubsection{Shifted-Invert Arnoldi Method}
The Shifted-Invert Arnoldi method is a technique that allows for better convergence on ill-conditioned system, given we know some information on the spectrum of $\mathbf{A}$. The whole idea is to combine two concepts that are common to Power Iterations (\cite{saad2011numerical}), namely Shifted Power and Inverted iteration. The idea is fairly simple, instead of applying Arnoldi iterations on $\mathbf{A}$, you will apply it to $(\mathbf{A}-\sigma\mathbf{I})^{-1}$ where $\sigma$ is a shift.

The interesting property about the shift is that it alters the eigenvalues but not the eigenvectors (\cite{saad2011numerical}). Once the eigenvalues $\mu$ of $(\mathbf{A}-\sigma\mathbf{I})^{-1}$ are computed, we can easily recover the eigenvalues of $\mathbf{A}$ by applying the shift $\sigma$ and inverting them
\begin{equation*}
    \lambda_i = \frac{1}{\mu_i} + \sigma
\end{equation*}
\begin{theorem}
    Say you chose an arbitrary $\sigma$, then given $\lambda_i$ and $\lambda_j$ two eigenvalues of $\mathbf{A}$ such that $\forall k \neq i,j$, $\|\sigma-\lambda_k\| > \|\sigma-\lambda_{i,j}\|$, then the convergence factor of Shifted-Invert method is given by 
    \begin{equation}
        \rho_{\mathbf{I}} = \frac{|\sigma-\lambda_i|}{|\sigma-\lambda_j|}
    \end{equation}
\end{theorem}
Consequently, we see that a good initialization of $\sigma$ is crucial to the convergence of the method. On the other end, a poor choice could have bad effects on the convergence, ultimately not converging at all. Several choices are open to us as for initilization of $\sigma$. One could stick with the unity, proceed to some power iterations, using the trace, or even use the Ritz values as computed in figure \ref{fig:ritz_poor_cond}. Inspired by the convergence rate of Power Iteration that is driven by the ratio of the two largest egeinvalues (in terms of magnitude), I suggest the following $\sigma$ as a good initializer:
\begin{equation}
    \sigma = \mathbb{E}\left[\frac{\lambda_1+\lambda_2}{2}\right]
\end{equation}
where $\lambda_1$ and $\lambda_2$ are the two largest eigenvalues of $\mathbf{A}$ in terms of magnitude. Computationally speaking, the Shifted-Invert procedure looks very heavy, especially considering the inverse operator : inverting such a large matrix is very costly. The good news is that we can take the LU factorization of $(\mathbf{A}-\sigma\mathbf{I})$, and then solve two linear system to compute the inverse. This is much more efficient than inverting the matrix directly. The Shifted-Invert Arnoldi method is described in Algorithm 5.
\begin{algorithm2e}
    \SetAlgoLined
    \KwData{$A \in \mathbb{C}^{n \times n}$, $v_1 \in \mathbb{C}^n$ a unit vector in the chosen norm, $\sigma\in\mathbb{C}$}
    \KwResult{$H_n \in \mathbb{C}^{n\times n}$}
    \caption{Shifted-Invert Arnoldi Iteration}
    $[L,U] \gets \text{lu}(A-\sigma I)$\;
    \For{$k = 1 \KwTo n$}{
        $\mathbf{w} \gets U^{-1}L^{-1}\mathbf{v}_k$\;
        \For{$i = 1 \KwTo k$}{
            $h_{ik} \gets \mathbf{v}_i^*\mathbf{w}$\;
            $\mathbf{w} \gets \mathbf{w} - h_{ik}\mathbf{v}_i$\;
        }
        $h_{k+1,k} \gets \|\mathbf{w}\|$\;
        \If{$h_{k+1,k} = 0$}{
            \textbf{break}\;
        }
        $\mathbf{v_{k+1}} \gets \mathbf{w}/h_{k+1,k}$
    }
    %\label{alg:arnoldi}
    %\label{alg:arnoldi}
\end{algorithm2e}
Note that in algorithm 5, $\sigma$ is not recomputed throughout the iterations. One way to potentially fasten the convergence, or to avoid having to define a good $\sigma$ initially (which can be costly) is to recompute $\sigma$ at each iteration (or at each $i$ iteration). A drawback obviously is that it requires to recompute the LU factorization, which is arguabely the most costly operation in Algorithm 4. We will not investigate this further in this note, but it is worth noting that this could be a potential improvement to the algorithm.

If we take a look at the convergence of the Ritz Values, similarly as in figure \ref{fig:fullritz}, we note that with this method, Ritz values converges no matter the matrix (figure \ref{fig:shiftritz_full}), and in much fewer iterations. We note that the final relative error is slightly higher than the machine precision $\epsilon$, and thus is slightly higher than in the regular Arnoldi Method, maybe because there is a numerical bias introduced by the shift and inverse steps. However, the relative error remains very small, around $10^{-14}$.

\begin{figure}
    \centering
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ritz_shift_bad.tex}
        \caption{}
        \label{fig:ritz_shift_bad}
    \end{subfigure}\hspace{0.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/ritz_shift_good.tex}
        \caption{}
        \label{fig:ritz_shift_good}
    \end{subfigure}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/sign_shift_bad.tex}
        \caption{}
        \label{fig:sign_shift_bad}
    \end{subfigure}\hspace{0.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/sign_shift_good.tex}
        \caption{}
        \label{fig:sign_shift_good}
    \end{subfigure}
    \caption{Same experiments as in figure \ref{fig:fullritz} using a static $\sigma=\mathbb{E}[(\lambda_1+\lambda_2)/2]$. We see that the convergence is much better than in the previous case, and that the Ritz values converge much faster in both matrices. The well-conditioned matrix (figures \ref{fig:ritz_shift_good} and \ref{fig:sign_shift_good}) also converges, but in much fewer iterations. Finally, wee see that even the very ill-conditioned matrix (figures \ref{fig:ritz_shift_bad} and \ref{fig:sign_shift_bad}) converges in only 20 iterations.}
    \label{fig:shiftritz_full}
\end{figure}
\subsubsection{Counting positive eigenvalues}
We now have an efficient arnoldi method for ill-conditioned matrices. Let us refine Algorithm 4 using the shifted invert Arnoldi method. To better assess the spectral component of the matrix, we will not use the same approach to define $\sigma$. Instead, $\sigma$ will be defined by a uniformly distributed variable such that $$\sigma\sim\mathcal{U}(lb,ub)$$ with 
\begin{equation*}
    \begin{cases}
        lb = \mathbb{E}\left[\min(\text{real}(\lambda_i))\right] \\
        ub = \mathbb{E}\left[\max(\text{real}(\lambda_i))\right]
    \end{cases}
\end{equation*}
This way, the shifted invert will, throughout the iteration, explore the whole spectrum of $\mathbf{A}$, and thus will be able to estimate the number of positive eigenvalues thanks to the convergence of the Monte-Carlo like approach. Algorithm 6 describes the new method.
\begin{algorithm2e}
    \SetAlgoLined
    \KwData{$A \in \mathbb{C}^{n \times n}$, $k \in \mathbb{N}$, $N \in \mathbb{N}$, $(lb,ub)\in\mathbb{R}^2$}
    \KwResult{$k_+ \in \mathbb{R}$}
    \caption{Corrected Computing $k_{+}(A)$}
    $\mathbf{q} \gets \mathbf{0}\in\mathbb{R}^{N}$\;
    \For{$i = 1$ \KwTo $N$}{
      $\mathbf{u} \gets \text{randn}(n, 1)$\;
      $\hat{\mathbf{u}} \gets \frac{\mathbf{u}}{\|\mathbf{u}\|}$\;
      $\sigma\gets \text{uniform}(lb,ub)$\;
      $[H, Q] \gets \text{shifted\_invert\_arnoldi}(A, \hat{\mathbf{u}}, k, \sigma)$\;
      $q(i) \gets \text{trace}(\text{sign}(H))$\;
    }
    $q_+ := \text{mode}(\mathbf{q}) $\;
    $k_+\gets (q+k)/2$
\end{algorithm2e}

In figure \ref{fig:kp}, we show the comparison between Algorithm 4 and 6. For this case, the maximum dimension of the Krylov subspace $k$ was set at 15, as we saw it provided good spectral approximation using the shifed-invert Arnoldi (figure \ref{fig:shiftritz_full}). The number of samples is fairly low, we vary this number from 1 to 20. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/wrong_kp.tex}
        \caption{}
        \label{fig:wrong_kp}
    \end{subfigure}\hspace{0.05\linewidth}
    \begin{subfigure}[b]{.45\linewidth}
        \input{figures/correct_kp.tex}
        \caption{}
        \label{fig:correct_kp}
    \end{subfigure}
    \caption{Comparison of the estimation of $k_+$ with Algorithm 4 and 6. We see that Algorithm 4 (figure \ref{fig:wrong_kp}) does not manage to find any positive eigenvalues. It uses the standard Arnoldi method, and we saw previously that it was not converging quickly enough. Whereas Algorithm 6 (figure \ref{fig:correct_kp}) finds the correct value of $k_p$ very quickly. $\hat{k}_+$ (continuous line) is the estimated value of $k_+$, and $k_+$ (dashed line) is the true value of $k_+$.}
    \label{fig:kp}
\end{figure}

We conclude that, assuming a good knowledge of the spectral properties of $\mathbf{A}$ (here, the bounds of its spectrum) enables for faster and more precise computations. In this case, it allows for quick assessment of a system's stability, which is a very important property in control theory. However, we note that the method we propose is computationally heavy, and can end up being very slow if $\mathbf{A}$ is poorly conditioned and if $k$ and $N$ are taken large.

\section{Conclusion}

This document has presented a comprehensive investigation into the theory, computation, and application of matrix functions. We started with a rigorous definition, exploring three distinct approaches: the natural definition, the spectrum-based definition, and the interpolation-based definition. This theoretical background was critical for extending the study into more complex computational realms (\cite{higham2008functions}).

The role of the matrix-vector product \(f(\mathbf{A})\mathbf{b}\) was subsequently analyzed, especially emphasizing its computational aspects. We delved into its importance across a wide range of numerical methods like Power Iteration, Conjugate Gradient Method, Lanczos Algorithm, and more (\cite{golub2013matrix}). A dedicated discussion on the computationally efficient evaluation of \(f(\mathbf{A})\mathbf{b}\) brought in concepts like the (Modified-Gram-Schmidt) Arnoldi Method, accentuating the role of Krylov subspaces (\cite{liesen2013krylov}).

Following this, we took an algorithmic stance, presenting the intricacies involved in implementing the computation of matrix functions in the dense case as well as when computing the matrix-vector product. Efficacy and efficiency were weighed through empirical results, providing a deeper understanding of algorithmic choices.

Finally, we discussed real-world applications, particularly focusing on matrix exponentials and sign functions. The document broadened the scope by introducing the Shifted-Invert Arnoldi Method as a nuanced approach for dealing with system stability, and we looked at how counting positive eigenvalues can offer significant insights into system characteristics.

\printbibliography

\section*{Supplementary Material}

To complete this note, and to allow for reproductibility, some Matlab codes are provided within the note's GitHub's repository : \href{https://github.com/nathan-rousselot/matrix-functions-krylov}{https://github.com/nathan-rousselot/matrix-functions-krylov}

The \texttt{Matlab} codes have been implemented in \texttt{Matalb R2023a Update 2}. Given the low-level nature of the codes it should work properly with reasonably older or newer versions. Note that \texttt{matrix\_function} rely on symbolic representation, a \texttt{Matlab} feature that allows for more precise function manipulations, and this feature is often revamped by Mathworks : this might be the function that is the most prone to become incompatible with other versions. If that is the case, you can modify it with a finite-difference estimation for the derivative, \textit{i.e} you define $h>\epsilon$ very small and compute the slope of the function on $h$.

All functions and demonstration files are in the \texttt{src/} folder. Below is a complete list of what is provided.
\begin{itemize}
    \item \texttt{arnoldi\_convergence($\mathbf{A},K$)} : this is a demonstration script that illustrates the rate of convergence of the Modified Gram-Shmidt Arnoldi method. It takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$ and a scalar $K$ that defines the maximum Krylov subspace dimension. It then plots the convergence of Arnoldi in function of the subspace dimension.
    \item \texttt{arnoldi\_iteration($\mathbf{A},\mathbf{v}_1,k$)} : this is the Modified Gram-Schmidt Arnoldi method as described in Algorithm 1. It takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, a vector $\mathbf{v}_1\in\mathbb{C}^n$ which should already satisfy $\|\mathbf{v}_1\|_2 = 1$ and a scalar $k$ that defines the dimension of the Krylov subspace such that $\mathcal{K}_k(\mathbf{A},\mathbf{v}_1) = \dots$. It returns two matrices $\mathbf{H}$ and $\mathbf{V}$ corresponding to the Hessenberg reduction of rank $k$ of $\mathbf{A}$
    \item \texttt{compute\_kp($\mathbf{A},k,N,lb,ub$)} : function that use a Monte-Carlo like method to estimate the number of positive eigenvalues in the matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$. $k$ is the dimension of the considered Krylov Subspace, $N$ is the number of samples, $lb,ub$ are the lower and uper bounds of the uniform distribution associated with $\sigma\sim\mathcal{U}(lb,ub)$. The function returns a scalar $kp$, the estimated positive eigenvalues.
    \item \texttt{convection\_diffusion()} : a standalone script that demonstrates the efficiency of the matrix-vector estimate. Illustrates the Convection-Diffusion problem.
    \item \texttt{demo\_compute\_kp($\mathbf{A},N,k$)} : demonstration function to illustrate the convergence (Monte-Carlo wise) of \texttt{compute\_kp}. It plots the convergence of the method with reference to the actual number of positive eigenvalues. It takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, a scalar $N$ that is the maximum number of Monte-Carlo sampling, and $k$ the considered Krylov subspace's dimension.
    \item \texttt{fAb($fun, \mathbf{A},\mathbf{b},k$)} : function that computationally efficiently evaluates $f(\mathbf{A})\mathbf{b}$ with $\mathbf{A}\in\mathbb{C}^{n\times n}$, $b\in\mathbb{C}^{n\times n}$. $fun$ is a function handle, and $k$ is the dimension of the Krylov subspace. It returns a vector $\mathbf{f}$.
    \item \texttt{generate\_diagonal\_matrices($n$)} : a simple function that takes a scalar $n$ as input and outputs a matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ diagonal whose entries $a_{ii}\sim\mathcal{U}(0,1)$.
    \item \texttt{generate\_symmetric\_matrices($n$)} a function that takes a scalar $n$ as input and outputs a matrix $\mathbf{A}\in\mathbb{R}^{n\times n}$ symmetric whose non-zero entries $a_{ij}\sim\mathcal{U}(0,1)$
    \item \texttt{matrix\_function($fun, \mathbf{A}$)} : a function that evaluates a given function handle $fun$ at the matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$. It uses Hermite Interpolation method as per definition \ref{def:Hermite}. It outputs a matrix $\mathbf{F}\in\mathbb{C}^{n\times n}$. Note that this function converts the input handle in symbolic representation. The entire purpose of the function is then to properly form the input for the Hermite Interpolation.
    \item \texttt{shifted\_arnoldi\_convergence($\mathbf{A},K$)} : this is a demonstration script that illustrates the rate of convergence of the Shifted-Invert Modified Gram-Shmidt Arnoldi method. It takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$ and a scalar $K$ that defines the maximum Krylov subspace dimension. It then plots the convergence of Arnoldi in function of the subspace dimension. Please note that $\sigma$ is not in the input, and is by default computed within the function such that $\sigma=\mathbb{E}[(\lambda_1+\lambda_2)/2]$. The function also plots the Ritz values with eigenvalues (the six biggest in terms of magnitude) to visually verify if Ritz values have converged.
    \item \texttt{shifted\_arnoldi\_iteration($\mathbf{A},\mathbf{v}_1,k,\sigma$)} : this is the Shited-Invert Modified Gram-Schmidt Arnoldi method as described in Algorithm 1. It takes as input a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, a vector $\mathbf{v}_1\in\mathbb{C}^n$ which should already satisfy $\|\mathbf{v}_1\|_2 = 1$ and a scalar $k$ that defines the dimension of the Krylov subspace such that $\mathcal{K}_k(\mathbf{A},\mathbf{v}_1) = \dots$. It also takes as input the initial shift $\sigma$. It uses computational efficient inversion by computing the LU factorization of $\mathbf{A}$ before proceeding to the inversion. It returns two matrices $\mathbf{H}$ and $\mathbf{V}$ corresponding to the Hessenberg reduction of rank $k$ of $\mathbf{A}$. It also returns $\sigma$, to track any change of $\sigma$. Note that this last output is currently of no use as for the proposed implementation, $\sigma$ is not changed throughout the iterations. Though only few lines of codes need to be changed for that to happen, in which case the output link is already done.
    \item \texttt{test1()} : this is a standalone script that demonstrates the efficiency and limitations of \texttt{matrix\_function}. In other words, it successively tries different function handles, namely $\cos, \sin$ and $\exp$ on a given matrix (\texttt{test1.mat}), then on differently structured random matrices. 
    \item \texttt{test\_fab()} : this is a standalone scipt that demonstrates the efficiency and limitations of \texttt{fAb}. It successively tries different function handles, namely $\cos, \sin$ and $\exp$ on a given matrix. The vector $\mathbf{b}$ is automatically and randomly generated such that $b_i\sim\mathcal{U}(0,1)$. The matrix on which the script is executed is to be set manually on line 9. 
\end{itemize}
Those functions and scripts have been conceived specifically for this note. Obviousy, they have dependencies written by other engineers or scientists. Those dependencies are included in the repository, though we do not document them.


\end{document}