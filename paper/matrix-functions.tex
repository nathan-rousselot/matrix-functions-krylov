%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Problem Set/Assignment Template to be used by the
%% Food and Resource Economics Department - IFAS
%% University of Florida's graduates.
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Version 1.0 - November 2019
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Ariel Soto-Caro
%%  - asotocaro@ufl.edu
%%  - arielsotocaro@gmail.com
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{float}
\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{tikz}
\usepackage[style=authoryear, backend=biber]{biblatex}
\bibliography{bibliography.bib}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
   \node[shape=circle,draw=red,inner sep=1pt] (char) {#1};}}
\setlength\parindent{0pt} %% Do not touch this
\DeclareMathOperator{\phiAb}{\phi_{A,\mathbf{b}}}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[theorem]

\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}

%% -----------------------------
%% TITLE
%% -----------------------------
\title{Matrix functions} %% Assignment Title
\author{Nathan Rousselot\\Mathematical Engineering Department\\KU Leuven}
%% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%\setlength{\droptitle}{-5em}    
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\section{Introduction}
In this document we introduce the notion of matrix functions. Say we have a function $f:\mathbb{C}\rightarrow\mathbb{C}$, then we can define the function $f$ on a matrix $\mathbf{A}$ as follows: $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$. In the following, we will first provide some theretical background on matrix function. The, we will describe the numerical issues coming with manipulating matrix functions. Finally, we will provide algorithm for efficient computation of matrix fuctions.

\section{Theoretical background}
\subsection{Natural Definition}
\subsubsection*{Polynomial functions}
Let $p:\mathbb{C}\rightarrow\mathbb{C}$ be a polynomial function of degree $d$:
\begin{equation}
    p(t) = \sum_{k=0}^d c_k t^k
\end{equation}
Then, considering a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and posing $\mathbf{A}^0 = I_n$, we can define the polynomial function $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ on a matrix $\mathbf{A}$ as follows:
\begin{equation}
    p(\mathbf{A}) = \sum_{k=0}^d c_k \mathbf{A}^k
\end{equation}

\subsubsection*{Rational functions}
Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a rational function of the form:
\begin{equation}
    f(t) := \frac{p(t)}{q(t)}
\end{equation}
It is not immediate how one would approach this function with a matrix. We want to define
\begin{equation}\label{eq:rationale}
    f(\mathbf{A}) := q(\mathbf{A})^{-1}p(\mathbf{A})
\end{equation}
However, this is not well defined if $q(\mathbf{A})$ is singular. In other words, we need to make sure that $q(\mathbf{A})$ is invertible. This is the case if and only if $q(\lambda) \neq 0$ for all eigenvalues $\lambda$ of $\mathbf{A}$. This is a very strong condition, and it is not always possible to find a rational function $f$ such that $q(\lambda) \neq 0$ for all eigenvalues $\lambda$ of $\mathbf{A}$. We note that a choice in notation has been made in equation \ref{eq:rationale}, the other notation is still valid.
\begin{lemma}\label{lem:commute}
     Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and let $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ and $q:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$, two matrix polynomials. Then,
     \begin{equation}
         q(\mathbf{A})p(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})
     \end{equation}
\end{lemma}
\begin{proof}
    Let $f(\mathbf{A}) := q(\mathbf{A})p(\mathbf{A})$, and $g(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})$. Then
    \begin{align*}
        f(\mathbf{A}) = \left(\sum_{k=0}^d c_k\mathbf{A}^k\right)\left(\sum_{j=0}^m b_j\mathbf{A}^j\right)\\
        = \sum_{k=0}^d \sum_{j=0}^m c_kb_j\mathbf{A}^{j+k}\\
        = \sum_{j=0}^m \sum_{k=0}^d b_jc_k\mathbf{A}^{k+j}\\
        = \left(\sum_{j=0}^m b_j\mathbf{A}^j\right)\left(\sum_{k=0}^d c_k\mathbf{A}^k\right) = g(\mathbf{A})
    \end{align*}
\end{proof}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and let $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ and $q:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$, two matrix polynomials such that $q(\mathbf{A})$ is non-singular. Then, 
    \begin{equation}
        q(\mathbf{A})^{-1}p(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})^{-1}
    \end{equation}
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a matrix such that $q(\mathbf{A})$ is non-singular
    \begin{align*}
        q(\mathbf{A})^{-1}p(\mathbf{A}) = q(\mathbf{A})^{-1}p(\mathbf{A})q(\mathbf{A})q(\mathbf{A})^{-1} 
    \end{align*}
    Using Lemma \ref{lem:commute}
    \begin{align*}        
    q(\mathbf{A})^{-1}p(\mathbf{A})q(\mathbf{A})q(\mathbf{A})^{-1} = q(\mathbf{A})^{-1}q(\mathbf{A})p(\mathbf{A})q(\mathbf{A})^{-1} \\
        = p(\mathbf{A})q(\mathbf{A})^{-1}
    \end{align*}
\end{proof}

\subsubsection*{Power Series}
Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a function that can be expressed as a power series:
\begin{equation}
    f(t) = \sum_{k=0}^\infty c_k t^k
\end{equation}
Then, considering a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and posing $\mathbf{A}^0 = I_n$, we can define the power series function $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ on a matrix $\mathbf{A}$ as follows:
\begin{equation}
    f(\mathbf{A}) = \sum_{k=0}^\infty c_k \mathbf{A}^k
\end{equation}
In the scalar case, we know that the power series converges if $|t| < r$, where $r$ is the radius of convergence. Obviously this translates in the matrix power series
\begin{theorem}\label{th:power_convergence}
    Let $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ be a matrix power series. Then, the series converges if and only if $\rho(\mathbf{A}) < r$, where $\rho(\mathbf{A})$ is the spectral radius of $\mathbf{A}$, and $r$ is the radius of convergence of the scalar power series.
\end{theorem}
Proof is provided in \cite{frommer2008matrix}. In the case of a finite-order Laurent Series, \textit{i.e}:
\begin{equation}
    f(t) = \sum_{k=-d}^d c_k t^k
\end{equation}
For the matrix case, we need to ensure convergence (similarly to power series), but also ensure existance of the inverse, as Laurent series do have negative powers. If both of those conditions are satisfied, we can write the Laurent series as a matrix function:
\begin{equation}
    f(\mathbf{A}) = \sum_{k=-d}^d c_k \mathbf{A}^k
\end{equation}
\subsection{Spectum-Based Definition}
\subsubsection*{Diagonalizable Matrices}
Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a diagonalizable matrix. That means that there exists a matrix $\mathbf{V}\in\mathbb{C}^{n\times n}$ such that $\mathbf{V}$ is invertible, and $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$, where $\mathbf{\Lambda}$ is a diagonal matrix containing the eigenvalues of $\mathbf{A}$ :
\begin{equation}
    \mathbf{\Lambda} = \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix}
\end{equation}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a diagonalizable matrix. Then we can define the function $f(\mathbf{A})$ as:
    \begin{equation}
        f(\mathbf{A}) := \mathbf{V}f(\mathbf{\Lambda})\mathbf{V}^{-1}
    \end{equation}
    with 
    \begin{equation}
        f(\mathbf{\Lambda}) = \begin{bmatrix}
            f(\lambda_1) & 0 & \cdots & 0 \\
            0 & f(\lambda_2) & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\lambda_n)
        \end{bmatrix}
    \end{equation}        
\end{theorem}
This property is very handy, as it allows us to compute matrix functions by simply applying the function to the eigenvalues of the matrix. Computationally, this avoids inverting matrices, and lots of matrix products. However, this property is only valid for diagonalizable matrices. This property puts constraints on $f(\mathbf{A})$, as its eigenvectors must form a basis $\mathbf{F}^n$. Another more practical constraint, that is sufficient but not necessary, is if $\mathbf{A}$ is a full rank matrix, then it is diagonalizable.

\subsubsection*{Defective Matrices}
In some cases, the matrix $\mathbf{A}$ is not diagonalizable, that means the sum of the dimensions of the eigenspaces is less than $n$, we call that a \textit{Defective Matrix}. In that case, we can generalize the principle of diagonalization using the Jordan canonical form of $\mathbf{A}$:
\begin{equation}
    \mathbf{A} = \mathbf{V}\mathbf{J}\mathbf{V}^{-1}
\end{equation}
where $\mathbf{J}$ is a Jordan matrix, and $\mathbf{V}$ is a matrix containing the generalized eigenvectors of $\mathbf{A}$. The Jordan matrix is a block diagonal matrix, where each block is a Jordan block. A Jordan block is a matrix of the form:
\begin{equation}
    \mathbf{J}_k(\lambda) = \begin{bmatrix}
        \lambda & 1 & 0 & \cdots & 0 \\
        0 & \lambda & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda & 1 \\
        0 & 0 & \cdots & 0 & \lambda
    \end{bmatrix}
\end{equation}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a defective matrix. Then we can define the function $f(\mathbf{A})$ as:
    \begin{equation}
        f(\mathbf{A}) := \mathbf{V}f(\mathbf{J})\mathbf{V}^{-1}
    \end{equation}
    with 
    \begin{equation}
        f(\mathbf{J}) = \begin{bmatrix}
            f(\mathbf{J}_1(\lambda_1)) & 0 & \cdots & 0 \\
            0 & f(\mathbf{J}_2(\lambda_2)) & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\mathbf{J}_k(\lambda_k))
        \end{bmatrix}
    \end{equation}
    and \begin{equation}
        f(\mathbf{J}_i(\lambda_i)) = \begin{bmatrix}
            f(\lambda_i) & f'(\lambda_i) & \frac{f''(\lambda_i)}{2!} & \cdots & \frac{f^{(k-1)}(\lambda_i)}{(k-1)!} \\
            0 & f(\lambda_i) & f'(\lambda_i) & \cdots & \frac{f^{(k-2)}(\lambda_i)}{(k-2)!} \\
            \vdots & \vdots & \ddots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\lambda_i) & f'(\lambda_i) \\
            0 & 0 & \cdots & 0 & f(\lambda_i)
        \end{bmatrix}
    \end{equation}
\end{theorem}
Obviously, both definitions of matrix functions, based on diagonalization and Jordan canonical form, presuppose that the spectral radius of $\mathbf{A}$, $\rho(\mathbf{A})$, is less than $r$, the radius of convergence.
\subsection{Interpolation-based definition}
Interestingly, in this section we will show that for any $\mathbf{A}\in\mathbb{C}^{n\times n}$ and any sufficiently differentiable $f$, we can find a polynomial $p$ such that $f(\mathbf{A})=p(\mathbf{A})$. First, let us observe from previous sections that only the eigenvalues of $\mathbf{A}$ are actually important for matrix polynomials. Also recall that every matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$ with spectrum $\{\lambda_1,\dots,\lambda_n\}$ has a minimal polynomial $\phi_{\mathbf{A}}$ given by
\begin{equation}\label{eq:minimalpoly}
\phi_{\mathbf{A}}(t):=\prod_{i=1}^{k}(t-\lambda_k)^{n_k}
\end{equation}
which is the unique monic minimal degree ($\text{degr}(\phi_{\mathbf{A}})=n_1+\cdots+n_k\leq n$) polynomial such that $\phi_{\mathbf{A}}(\mathbf{A})=0$.
\begin{theorem}\label{thm:unicity}
Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. Then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})=p_2(\mathbf{A})$ if and only if 
$$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j).$$
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, with spectrum $\{\lambda_1,\ldots,\lambda_k\}$, and two polynomials $p_1$ and $p_2$ such that $p_1(\mathbf{A})=p_2(\mathbf{A})$. Let us define $q$ such that
    \begin{equation*}
        q := p_1 - p_2
    \end{equation*}
    Then, $q(\mathbf{A})=0$, and is thus divisible by $\phi_{\mathbf{A}}$, meaning that
    \begin{equation*}
        \forall j\in\{1,\dots,k\}:\forall i\in\{0,\ldots,n_k-1\}, q(\lambda_j) = 0 \Rightarrow p_1^{(i)}(\lambda_j) = p_1^{(i)}(\lambda_i)
    \end{equation*}
    Similarly, consider two polynomials $p_1$ and $p_2$ such that 
    \begin{equation*}
        \forall j\in\{1,\dots,k\}:\forall i\in\{0,\ldots,n_k-1\},  p_1^{(i)}(\lambda_j) = p_2^{(i)}(\lambda_i)    
    \end{equation*}
    For $i=0$, $q:=p_1-p_2=0$ on the spectrum of $\mathbf{A}$, and is then divisible by $\phi_{\mathbf{A}}$. Then
    \begin{equation*}
        q = K\phi_{\mathbf{A}}
    \end{equation*}
    with $K$ a polynomial. Then, $q(\mathbf{A})=K(\mathbf{A})\phi_{\mathbf{A}}(\mathbf{A}) = 0$ since by definition, $\phi_{\mathbf{A}}(\mathbf{A}) = 0$. And thus, $p_1(\mathbf{A})=p_2(\mathbf{A})$.

    From this reasoning, we conclude that 
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
\end{proof}
In theorem~\ref{thm:unicity}, the conditions involve the evaluation of the polynomials $p_1$ and $p_2$ and their derivatives up to order $n_k-1$ at the eigenvalues of $A$.

However, when the spectrum of $A$ is simple, each eigenvalue $\lambda_j$ is of multiplicity $n_j=1$. This means that there are no higher order terms corresponding to these eigenvalues in the minimal polynomial, or in other words, there are no repeated roots. Consequently, there is no need to consider the derivatives of the polynomials $p_1$ and $p_2$ because there are no repeated roots for the polynomials to ``match up'' with. Therefore, in this simpler case, we only need to check that the polynomials $p_1$ and $p_2$ agree at the eigenvalues of $A$. In formal terms, the condition becomes:
\begin{corollary}\label{cor:unicity}
Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. If $\mathbf{A}$ has simple spectrum, then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})=p_2(\mathbf{A})$ if and only if
\begin{equation*}
    \forall j\in\{1,\ldots,k\}: p_1(\lambda_j)=p_2(\lambda_j).
\end{equation*}
\end{corollary}
From this corollary, and from theorem \ref{thm:unicity}, we can confirm our earlier statement : only the spectrum of $\mathbf{A}$ is important for matrix polynomials. More importantly, we observe that $p(\mathbf{A})$ is uniquely defined by its values on the spectrum of $\mathbf{A}$. It seems then natural to extend this definition to any function $f$.
\begin{definition}\label{def:Hermite}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a matrix with minimal polynomial given as in equation \ref{eq:minimalpoly}, and let $f$ be a function that is at least $\max_k\{n_k-1\}$ times differentiable. Say $p$ is its $(n_1,\ldots,n_k)$-\emph{Hermite interpolant} i.e. the polynomial satisfying
    $$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p^{(i)}(\lambda_j)=f^{(i)}(\lambda_j)$$
    of minimal degree. Then we define $f(\mathbf{A})=p(\mathbf{A})$.
\end{definition}
%%%%%%% NEED TO COMMENT ON THIS %%%%%%%%
\printbibliography
\end{document}