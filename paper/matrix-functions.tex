%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Problem Set/Assignment Template to be used by the
%% Food and Resource Economics Department - IFAS
%% University of Florida's graduates.
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Version 1.0 - November 2019
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Ariel Soto-Caro
%%  - asotocaro@ufl.edu
%%  - arielsotocaro@gmail.com
%% %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt]{article}
\usepackage{float}
\usepackage[ruled,vlined,linesnumbered,algo2e]{algorithm2e}
\usepackage{amsmath,amssymb}
\usepackage{amsthm}
\usepackage{makecell}
\usepackage{tikz}
\usepackage[style=authoryear, backend=biber]{biblatex}
\bibliography{bibliography.bib}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
   \node[shape=circle,draw=red,inner sep=1pt] (char) {#1};}}
\setlength\parindent{0pt} %% Do not touch this
\DeclareMathOperator{\phiAb}{\phi_{\mathbf{A},\mathbf{b}}}
\newtheorem{lemma}{Lemma}
\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newtheorem{corollary}{Corollary}[theorem]

\renewcommand{\theequation}{\arabic{section}.\arabic{equation}}

%% -----------------------------
%% TITLE
%% -----------------------------
\title{Matrix functions} %% Assignment Title
\author{Nathan Rousselot\\Mathematical Engineering Department\\KU Leuven}
%% Change "\today" by another date manually
%% -----------------------------
%% -----------------------------

%% %%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%\setlength{\droptitle}{-5em}    
%% %%%%%%%%%%%%%%%%%%%%%%%%%
\maketitle

\section{Introduction}
In this document we introduce the notion of matrix functions. Say we have a function $f:\mathbb{C}\rightarrow\mathbb{C}$, then we can define the function $f$ on a matrix $\mathbf{A}$ as follows: $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$. In the following, we will first provide some theretical background on matrix function. The, we will describe the numerical issues coming with manipulating matrix functions. Finally, we will provide algorithm for efficient computation of matrix fuctions.

\section{Theoretical background}
\subsection{Natural Definition}
\subsubsection*{Polynomial functions}
Let $p:\mathbb{C}\rightarrow\mathbb{C}$ be a polynomial function of degree $d$:
\begin{equation}
    p(t) = \sum_{k=0}^d c_k t^k
\end{equation}
Then, considering a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and posing $\mathbf{A}^0 = I_n$, we can define the polynomial function $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ on a matrix $\mathbf{A}$ as follows:
\begin{equation}
    p(\mathbf{A}) = \sum_{k=0}^d c_k \mathbf{A}^k
\end{equation}

\subsubsection*{Rational functions}
Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a rational function of the form:
\begin{equation}
    f(t) := \frac{p(t)}{q(t)}
\end{equation}
It is not immediate how one would approach this function with a matrix. We want to define
\begin{equation}\label{eq:rationale}
    f(\mathbf{A}) := q(\mathbf{A})^{-1}p(\mathbf{A})
\end{equation}
However, this is not well defined if $q(\mathbf{A})$ is singular. In other words, we need to make sure that $q(\mathbf{A})$ is invertible. This is the case if and only if $q(\lambda) \neq 0$ for all eigenvalues $\lambda$ of $\mathbf{A}$. This is a very strong condition, and it is not always possible to find a rational function $f$ such that $q(\lambda) \neq 0$ for all eigenvalues $\lambda$ of $\mathbf{A}$. We note that a choice in notation has been made in equation \ref{eq:rationale}, the other notation is still valid.
\begin{lemma}\label{lem:commute}
     Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and let $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ and $q:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$, two matrix polynomials. Then,
     \begin{equation}
         q(\mathbf{A})p(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})
     \end{equation}
\end{lemma}
\begin{proof}
    Let $f(\mathbf{A}) := q(\mathbf{A})p(\mathbf{A})$, and $g(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})$. Then
    \begin{align*}
        f(\mathbf{A}) = \left(\sum_{k=0}^d c_k\mathbf{A}^k\right)\left(\sum_{j=0}^m b_j\mathbf{A}^j\right)\\
        = \sum_{k=0}^d \sum_{j=0}^m c_kb_j\mathbf{A}^{j+k}\\
        = \sum_{j=0}^m \sum_{k=0}^d b_jc_k\mathbf{A}^{k+j}\\
        = \left(\sum_{j=0}^m b_j\mathbf{A}^j\right)\left(\sum_{k=0}^d c_k\mathbf{A}^k\right) = g(\mathbf{A})
    \end{align*}
\end{proof}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, and let $p:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ and $q:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$, two matrix polynomials such that $q(\mathbf{A})$ is non-singular. Then, 
    \begin{equation}
        q(\mathbf{A})^{-1}p(\mathbf{A}) = p(\mathbf{A})q(\mathbf{A})^{-1}
    \end{equation}
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a matrix such that $q(\mathbf{A})$ is non-singular
    \begin{align*}
        q(\mathbf{A})^{-1}p(\mathbf{A}) = q(\mathbf{A})^{-1}p(\mathbf{A})q(\mathbf{A})q(\mathbf{A})^{-1} 
    \end{align*}
    Using Lemma \ref{lem:commute}
    \begin{align*}        
    q(\mathbf{A})^{-1}p(\mathbf{A})q(\mathbf{A})q(\mathbf{A})^{-1} = q(\mathbf{A})^{-1}q(\mathbf{A})p(\mathbf{A})q(\mathbf{A})^{-1} \\
        = p(\mathbf{A})q(\mathbf{A})^{-1}
    \end{align*}
\end{proof}

\subsubsection*{Power Series}
Let $f:\mathbb{C}\rightarrow\mathbb{C}$ be a function that can be expressed as a power series:
\begin{equation}
    f(t) = \sum_{k=0}^\infty c_k t^k
\end{equation}
Then, considering a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$, and posing $\mathbf{A}^0 = I_n$, we can define the power series function $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ on a matrix $\mathbf{A}$ as follows:
\begin{equation}
    f(\mathbf{A}) = \sum_{k=0}^\infty c_k \mathbf{A}^k
\end{equation}
In the scalar case, we know that the power series converges if $|t| < r$, where $r$ is the radius of convergence. Obviously this translates in the matrix power series
\begin{theorem}\label{th:power_convergence}
    Let $f:\mathbb{C}^{n\times n}\rightarrow\mathbb{C}^{n\times n}$ be a matrix power series. Then, the series converges if and only if $\rho(\mathbf{A}) < r$, where $\rho(\mathbf{A})$ is the spectral radius of $\mathbf{A}$, and $r$ is the radius of convergence of the scalar power series.
\end{theorem}
Proof is provided in \cite{frommer2008matrix}. In the case of a finite-order Laurent Series, \textit{i.e}:
\begin{equation}
    f(t) = \sum_{k=-d}^d c_k t^k
\end{equation}
For the matrix case, we need to ensure convergence (similarly to power series), but also ensure existance of the inverse, as Laurent series do have negative powers. If both of those conditions are satisfied, we can write the Laurent series as a matrix function:
\begin{equation}
    f(\mathbf{A}) = \sum_{k=-d}^d c_k \mathbf{A}^k
\end{equation}
\subsection{Spectum-Based Definition}
\subsubsection*{Diagonalizable Matrices}
Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a diagonalizable matrix. That means that there exists a matrix $\mathbf{V}\in\mathbb{C}^{n\times n}$ such that $\mathbf{V}$ is invertible, and $\mathbf{A} = \mathbf{V}\mathbf{\Lambda}\mathbf{V}^{-1}$, where $\mathbf{\Lambda}$ is a diagonal matrix containing the eigenvalues of $\mathbf{A}$ :
\begin{equation}
    \mathbf{\Lambda} = \begin{bmatrix}
        \lambda_1 & 0 & \cdots & 0 \\
        0 & \lambda_2 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda_n
    \end{bmatrix}
\end{equation}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a diagonalizable matrix. Then we can define the function $f(\mathbf{A})$ as:
    \begin{equation}
        f(\mathbf{A}) := \mathbf{V}f(\mathbf{\Lambda})\mathbf{V}^{-1}
    \end{equation}
    with 
    \begin{equation}
        f(\mathbf{\Lambda}) = \begin{bmatrix}
            f(\lambda_1) & 0 & \cdots & 0 \\
            0 & f(\lambda_2) & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\lambda_n)
        \end{bmatrix}
    \end{equation}        
\end{theorem}
This property is very handy, as it allows us to compute matrix functions by simply applying the function to the eigenvalues of the matrix. Computationally, this avoids inverting matrices, and lots of matrix products. However, this property is only valid for diagonalizable matrices. This property puts constraints on $f(\mathbf{A})$, as its eigenvectors must form a basis $\mathbf{F}^n$. Another more practical constraint, that is sufficient but not necessary, is if $\mathbf{A}$ is a full rank matrix, then it is diagonalizable.

\subsubsection*{Defective Matrices}
In some cases, the matrix $\mathbf{A}$ is not diagonalizable, that means the sum of the dimensions of the eigenspaces is less than $n$, we call that a \textit{Defective Matrix}. In that case, we can generalize the principle of diagonalization using the Jordan canonical form of $\mathbf{A}$:
\begin{equation}
    \mathbf{A} = \mathbf{V}\mathbf{J}\mathbf{V}^{-1}
\end{equation}
where $\mathbf{J}$ is a Jordan matrix, and $\mathbf{V}$ is a matrix containing the generalized eigenvectors of $\mathbf{A}$. The Jordan matrix is a block diagonal matrix, where each block is a Jordan block. A Jordan block is a matrix of the form:
\begin{equation}
    \mathbf{J}_k(\lambda) = \begin{bmatrix}
        \lambda & 1 & 0 & \cdots & 0 \\
        0 & \lambda & 1 & \cdots & 0 \\
        \vdots & \vdots & \ddots & \ddots & \vdots \\
        0 & 0 & \cdots & \lambda & 1 \\
        0 & 0 & \cdots & 0 & \lambda
    \end{bmatrix}
\end{equation}
\begin{theorem}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a defective matrix. Then we can define the function $f(\mathbf{A})$ as:
    \begin{equation}
        f(\mathbf{A}) := \mathbf{V}f(\mathbf{J})\mathbf{V}^{-1}
    \end{equation}
    with 
    \begin{equation}
        f(\mathbf{J}) = \begin{bmatrix}
            f(\mathbf{J}_1(\lambda_1)) & 0 & \cdots & 0 \\
            0 & f(\mathbf{J}_2(\lambda_2)) & \cdots & 0 \\
            \vdots & \vdots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\mathbf{J}_k(\lambda_k))
        \end{bmatrix}
    \end{equation}
    and \begin{equation}
        f(\mathbf{J}_i(\lambda_i)) = \begin{bmatrix}
            f(\lambda_i) & f'(\lambda_i) & \frac{f''(\lambda_i)}{2!} & \cdots & \frac{f^{(k-1)}(\lambda_i)}{(k-1)!} \\
            0 & f(\lambda_i) & f'(\lambda_i) & \cdots & \frac{f^{(k-2)}(\lambda_i)}{(k-2)!} \\
            \vdots & \vdots & \ddots & \ddots & \vdots \\
            0 & 0 & \cdots & f(\lambda_i) & f'(\lambda_i) \\
            0 & 0 & \cdots & 0 & f(\lambda_i)
        \end{bmatrix}
    \end{equation}
\end{theorem}
Obviously, both definitions of matrix functions, based on diagonalization and Jordan canonical form, presuppose that the spectral radius of $\mathbf{A}$, $\rho(\mathbf{A})$, is less than $r$, the radius of convergence.
\subsection{Interpolation-based definition}
Interestingly, in this section we will show that for any $\mathbf{A}\in\mathbb{C}^{n\times n}$ and any sufficiently differentiable $f$, we can find a polynomial $p$ such that $f(\mathbf{A})=p(\mathbf{A})$. First, let us observe from previous sections that only the eigenvalues of $\mathbf{A}$ are actually important for matrix polynomials. Also recall that every matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$ with spectrum $\{\lambda_1,\dots,\lambda_n\}$ has a minimal polynomial $\phi_{\mathbf{A}}$ given by
\begin{equation}\label{eq:minimalpoly}
\phi_{\mathbf{A}}(t):=\prod_{i=1}^{k}(t-\lambda_k)^{n_k}
\end{equation}
which is the unique monic minimal degree ($\text{degr}(\phi_{\mathbf{A}})=n_1+\cdots+n_k\leq n$) polynomial such that $\phi_{\mathbf{A}}(\mathbf{A})=0$.
\begin{theorem}\label{thm:unicity}
Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. Then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})=p_2(\mathbf{A})$ if and only if 
$$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j).$$
\end{theorem}
\begin{proof}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, with spectrum $\{\lambda_1,\ldots,\lambda_k\}$, and two polynomials $p_1$ and $p_2$ such that $p_1(\mathbf{A})=p_2(\mathbf{A})$. Let us define $q$ such that
    \begin{equation*}
        q := p_1 - p_2
    \end{equation*}
    Then, $q(\mathbf{A})=0$, and is thus divisible by $\phi_{\mathbf{A}}$, meaning that
    \begin{equation*}
        \forall j\in\{1,\dots,k\}:\forall i\in\{0,\ldots,n_k-1\}, q(\lambda_j) = 0 \Rightarrow p_1^{(i)}(\lambda_j) = p_1^{(i)}(\lambda_i)
    \end{equation*}
    Similarly, consider two polynomials $p_1$ and $p_2$ such that 
    \begin{equation*}
        \forall j\in\{1,\dots,k\}:\forall i\in\{0,\ldots,n_k-1\},  p_1^{(i)}(\lambda_j) = p_2^{(i)}(\lambda_i)    
    \end{equation*}
    For $i=0$, $q:=p_1-p_2=0$ on the spectrum of $\mathbf{A}$, and is then divisible by $\phi_{\mathbf{A}}$. Then
    \begin{equation*}
        q = K\phi_{\mathbf{A}}
    \end{equation*}
    with $K$ a polynomial. Then, $q(\mathbf{A})=K(\mathbf{A})\phi_{\mathbf{A}}(\mathbf{A}) = 0$ since by definition, $\phi_{\mathbf{A}}(\mathbf{A}) = 0$. And thus, $p_1(\mathbf{A})=p_2(\mathbf{A})$.

    From this reasoning, we conclude that 
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
\end{proof}
In theorem~\ref{thm:unicity}, the conditions involve the evaluation of the polynomials $p_1$ and $p_2$ and their derivatives up to order $n_k-1$ at the eigenvalues of $A$.

However, when the spectrum of $A$ is simple, each eigenvalue $\lambda_j$ is of multiplicity $n_j=1$. This means that there are no higher order terms corresponding to these eigenvalues in the minimal polynomial, or in other words, there are no repeated roots. Consequently, there is no need to consider the derivatives of the polynomials $p_1$ and $p_2$ because there are no repeated roots for the polynomials to ``match up'' with. Therefore, in this simpler case, we only need to check that the polynomials $p_1$ and $p_2$ agree at the eigenvalues of $A$. In formal terms, the condition becomes:
\begin{corollary}\label{cor:unicity}
Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. If $\mathbf{A}$ has simple spectrum, then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})=p_2(\mathbf{A})$ if and only if
\begin{equation*}
    \forall j\in\{1,\ldots,k\}: p_1(\lambda_j)=p_2(\lambda_j).
\end{equation*}
\end{corollary}
From this corollary, and from theorem \ref{thm:unicity}, we can confirm our earlier statement : only the spectrum of $\mathbf{A}$ is important for matrix polynomials. More importantly, we observe that $p(\mathbf{A})$ is uniquely defined by its values on the spectrum of $\mathbf{A}$. It seems then natural to extend this definition to any function $f$.
\begin{definition}\label{def:Hermite}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$ be a matrix with minimal polynomial given as in equation \ref{eq:minimalpoly}, and let $f$ be a function that is at least $\max_k\{n_k-1\}$ times differentiable. Say $p$ is its $(n_1,\ldots,n_k)$-\emph{Hermite interpolant} i.e. the polynomial satisfying
    $$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p^{(i)}(\lambda_j)=f^{(i)}(\lambda_j)$$
    of minimal degree. Then we define $f(\mathbf{A})=p(\mathbf{A})$.
\end{definition}
%%%%%%% NEED TO COMMENT ON THIS %%%%%%%%
\section{The matrix-vector product $f(\mathbf{A})\mathbf{b}$}
\subsection{Introduction}\label{sec:fabintro}
To motivate the need for a matrix-vector product, we will consider a matrix $\mathbf{A}\in\mathbb{C}^{n\times n}$. Let us consider the matrix exponential, such as :
\begin{equation}
    e^{\mathbf{A}} = \sum_{k=0}^\infty \frac{\mathbf{A}^k}{k!}
\end{equation}
Not only does this computation is very heavy (see section \ref{sec:matrixexp} for computation strategies), but it also affects the structure of the matrix. Say for example, we have the following laplacian matrix: 
\begin{equation*}
    \mathbf{A} = \begin{bmatrix}
        2 & -1 & 0 & 0 & \dots &  \dots & 0 \\
        -1 & 2 & -1 & 0 & 0 & \dots & 0 \\
        0 & -1 & 2 & -1 & 0 & \dots & 0 \\
        \vdots & \vdots & \ddots & \ddots & \ddots & \ddots & \vdots \\
        0 & 0 & \dots & 0 & -1 & 2 & -1 \\
        0 & 0 & \dots & 0 & 0 & -1 & 2
    \end{bmatrix}
\end{equation*}
Obviously, $\mathbf{A}$ is a sparse matrix, and also a rank-structured one : it is a tridiagonal matrix. However, by computing $e^{\mathbf{A}}$, one will get a dense matrix. Storing a dense matrix often becomes challenging as the dimension of the problem grows. Besides storing, computing such a dense matrix is also an issue. However, the matrix-vector product $f(\mathbf{A})\mathbf{b}$ can be stored and computed efficiently.

\subsection{The Method}
As motivated by \ref{sec:fabintro}, when encountering specific structure in $\mathbf{A}$, such as sparsity, there is a lot to gain if we can store $f(\mathbf{A})\mathbf{b}$. In this section we will work towards a way to evaluate $f(\mathbf{A})\mathbf{b}$ in an intuitive way, thanks to Arnoldi method.

\subsubsection{Formal Definitions}
Firstly, we will define the notion of $\phiAb$, i.e. the minimal polynomial of $\mathbf{A}$ with respect to the vector $\mathbf{b}$. This is simply the polynomial
\begin{equation}\label{eq:minPolyAb}
\phiAb(t):= \prod_{i=1}^{k}(t-\lambda_i)^{m_k}
\end{equation}
of minimal degree such that $\phiAb(\mathbf{A})\mathbf{b}=\mathbf{0}$. Here $\lambda_1,\ldots,\lambda_k$ are again the eigenvalues of $\mathbf{A}$.

%%%%% EXAMPLE WHERE PHIab =/= PHIa %%%%%

\begin{lemma}\label{lem:phiAb}
    Let $\mathbf{A}\in \mathbb{C}^{n\times n}$ be a matrix with eigenvalues $\{\lambda_1,\ldots,\lambda_k\}$, and minimal polynomial given by equation~\ref{eq:minimalpoly}. Then for any two polynomials $p_1,p_2$ we have that $p_1(\mathbf{A})\mathbf{b}=p_2(\mathbf{A})\mathbf{b}$ if and only if 
$$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j).$$
\end{lemma}

\begin{proof}
    From theorem \ref{thm:unicity}, we know that
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
    We also know that 
    \begin{align*}
        p_1(\mathbf{A}) = p_2(\mathbf{A}) \Leftrightarrow p_1(\mathbf{A})\mathbf{b} = p_2(\mathbf{A})\mathbf{b}
    \end{align*}
    Thus, we conclude that
    \begin{align*}
        p_1(\mathbf{A})\mathbf{b} = p_2(\mathbf{A})\mathbf{b} \\ \Leftrightarrow \forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,n_k-1\}: p_1^{(i)}(\lambda_j)=p_2^{(i)}(\lambda_j)
    \end{align*}
\end{proof}


\begin{theorem}\label{thm:KrylovApprox}
    Let $f$ be a sufficiently differentiable function that has no singularities on the spectrum of a given matrix $\mathbf{A}\in\mathbb{C}^{n \times n}$. Then, with $p$ the unique Hermite interpolating polynomial of $\mathbf{A}$ w.r.t. $\mathbf{b}$ i.e. 
    $$\forall j\in\{1,\ldots,k\}:\forall i\in\{0,\ldots,m_k-1\}:p^{(i)}(\lambda_j)=f^{(i)}(\lambda_j)$$
    we have that $f(\mathbf{A})\mathbf{b}=p(\mathbf{A})\mathbf{b}$.
\end{theorem}
%%%%% PROVE IT %%%%%

\begin{proof}
    Let $f$ be a function that is at least $\max_k\{m_k-1\}$ times differentiable, and let $p_1$ be its $(m_1,\ldots,m_k)$-Hermite interpolant. Then, by definition \ref{def:Hermite}, we have that 
    \begin{equation*}
        f(\mathbf{A}) = p_1(\mathbf{A})
    \end{equation*}
    Then, by lemma \ref{lem:phiAb}, let us define $p_2$ such that
    \begin{equation*}
        p_2(\mathbf{A})\mathbf{b} = p_1(\mathbf{A})\mathbf{b}
    \end{equation*}
    Then, we have that
    \begin{equation*}
        p_1(\mathbf{A})\mathbf{b} = p_2(\mathbf{A})\mathbf{b} = f(\mathbf{A})\mathbf{b}
    \end{equation*}
\end{proof}

\subsubsection{The Arnoldi Method}
The Arnoldi method is a reduction method that allows low-rank approximation of a given matrix $\mathbf{A}$. It is based on the Hessenberg reduction of a matrix $\mathbf{A}$. More formally
\begin{definition}
    Let $\mathbf{A}\in\mathbb{C}^{n\times n}$, the Arnoldi method approximates its Hessenberg reduction given by
    \begin{equation}
        \mathbf{Q}^*\mathbf{A}\mathbf{Q} = \mathbf{H}
    \end{equation}
    where $\mathbf{Q}\in\mathbb{C}^{n\times n}$ is unitary, and $\mathbf{H}\in\mathbb{C}^{n\times n}$ is upper Hessenberg. As it is a (low-rank) approximation, Arnoldi will compute the following decomposition
    \begin{equation}
        \mathbf{Q}_k^*\mathbf{A}\mathbf{Q}_k = \mathbf{H}_k
    \end{equation}
    with $\mathbf{Q}_k\in\mathbb{C}^{n\times k}$ unitary, and $\mathbf{H}_k\in\mathbb{C}^{k\times k}$ upper Hessenberg. That means that $\mathbf{H}_k$ is the orthogonal projection of $\mathbf{A}$ onto the Krylov subspace $\mathcal{K}_k(\mathbf{A},\mathbf{q_1})$, with $\mathbf{q_1}$ the first column of $\mathbf{Q}$.
\end{definition}
\section{Algorithms}
In this section, we will implement basic algorithms. From the previous section, we will distinguish two cases : 
\begin{itemize}
    \item the dense case, where $f(\mathbf{A})$ is to be computed
    \item and the case where only the matrix-vector product $f(\mathbf{A})\mathbf{b}$ is to be computed
\end{itemize}
\subsection{Dense case}
\section{Applications}
\subsection{Matrix Exponential}\label{sec:matrixexp}
Consider the simple system of ODEs
\begin{equation}\label{eq:ode}
    \frac{d\mathbf{x}}{dt} = \mathbf{A}\mathbf{x}
\end{equation}
with initial condition $\mathbf{x}(0) = \mathbf{x}_0\in\mathbb{R}^n$. Then we know the solution to be given by $\mathbf{x}(t)=e^{At}\mathbf{x}_0$. However, for all but the stablest systems, this is not a good method, due to issues such as stability and stiffness.
\printbibliography
\end{document}